{"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9511835,"sourceType":"datasetVersion","datasetId":5790004},{"sourceId":992,"sourceType":"modelInstanceVersion","modelInstanceId":846,"modelId":101},{"sourceId":6080,"sourceType":"modelInstanceVersion","modelInstanceId":4592,"modelId":2796},{"sourceId":64146,"sourceType":"modelInstanceVersion","modelInstanceId":53489,"modelId":73369},{"sourceId":124095,"sourceType":"modelInstanceVersion","modelInstanceId":104448,"modelId":128647}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initialization","metadata":{"id":"gTvrxbQdeCRM"}},{"cell_type":"code","source":"import time\n# Unique indentifier specific to this execution\nrun_id = int(time.time())","metadata":{"id":"es8kyxsCe2rj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installations","metadata":{"id":"G6DGt7m9Mdey"}},{"cell_type":"code","source":"#! pip install tensorflow==2.15.0\n#! pip install -q imblearn keras-tuner tensorflow-addons keras-cv memory_profiler","metadata":{"id":"CLPR-TDUyM_1","outputId":"ec7aa160-b7c7-4053-a6d1-77a261747664","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global variables","metadata":{"id":"qRXYfz1Fe2rj"}},{"cell_type":"code","source":"# Database configuration: Choose between 'dagshub_storage', 'google_drive', 'local'\ndatabase = 'local'  # Current choice: local\n\n# Execution mode: Choose between 'CPU' or 'GPU'\nexecution_mode = 'GPU'  # Current choice: GPU\n\n# Execution environment: Choose between 'colab', 'mac'\nexecution_env = 'kaggle'  # Current choice: colab\n\n# Experimentation mode: Choose between 'test' or 'prod'\nexperimentation_mode = 'prod'  # Current choice: test\n\n# Training and Inference settings\ntrain_all_models = False  # Train all models\nrun_inference = True  # Run inference\nrun_inference_use_all_models = True  # Run inference on all models\nmodel_type = 'pretrained_ensemble_model'  # Type of model to train, choose between 'single_model', 'pretrained_ensemble_model', 'compact_ensemble_model'\ninference_model_name = model_type  # Use the model_type as the inference model name\n\n# Dataset settings\ndownload_raw_datasets = False  # Do not download raw datasets\ndownload_refined_training_datasets = True if execution_env == 'colab' else False  # Download refined datasets if running on Colab\ndataset_size = 20000  # Set dataset size to 40,000 samples\ncreate_new_training_datasets = False  # Do not create new training datasets","metadata":{"id":"MRVQhcz0puop","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Execution strategy","metadata":{"id":"6MvxojqUWoi5"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import mixed_precision\nimport warnings\n\ndef limit_memory_growth():\n    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n    try:\n        for gpu in gpu_devices:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except Exception as e:\n        print('Can not set memory growth', e)\n\ndef disable_eager_execution():\n    tf.compat.v1.disable_eager_execution()\n\ndef enable_xla():\n    tf.config.optimizer.set_jit(True)\n\ndef disable_gpu():\n    tf.config.set_visible_devices([], 'GPU')\n    visible_devices = tf.config.get_visible_devices()\n    for device in visible_devices:\n        assert device.device_type != 'GPU'\n\ndef enable_mixed_precision():\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    print(\"Mixed precision enabled: \", mixed_precision.global_policy())\n\n# Suppress all warnings in the notebook\nwarnings.filterwarnings('ignore')\n\nlimit_memory_growth()\n#enable_mixed_precision()\n#tf.debugging.set_log_device_placement(True)\n#tf.config.set_visible_devices([], 'GPU')\nenable_xla()\ntf.config.optimizer.set_experimental_options({\n    \"memory_optimization\": True\n})","metadata":{"id":"cU1NZXVce2rj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_tpu_initialized():\n    \"\"\"Check if TPU is already initialized.\"\"\"\n    tpu_devices = tf.config.list_logical_devices('TPU')\n    if len(tpu_devices) > 0:\n        print(f\"TPU is already initialized with {len(tpu_devices)} logical devices.\")\n        return True\n    else:\n        print(\"TPU is not initialized.\")\n        return False\n\ndef set_tpu_strategy(force=False):\n    \"\"\"Set the TPU strategy if TPU is initialized.\"\"\"\n    if check_tpu_initialized() and not force:\n        try:\n            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()   # TPU detection\n            strategy = tf.distribute.TPUStrategy(resolver)\n\n            print(\"Using TPU strategy.\")\n            return strategy\n        except Exception as e:\n            print(f\"Error while setting TPU strategy: {e}\")\n            return None\n    else:\n        try:\n            print(\"Trying to initialize TPU.\")\n            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(resolver)\n            tf.tpu.experimental.initialize_tpu_system(resolver)\n            strategy = tf.distribute.TPUStrategy(resolver)\n            print(\"TPU initialized and TPU strategy set.\")\n            return strategy\n        except ValueError:\n            print(\"No TPU devices found.\")\n            return None\n\nstrategy = set_tpu_strategy(force = execution_env == 'colab')\nrunning_on_tpu = strategy is not None\n\nif strategy is None:\n    # For GPUs or CPU\n    if tf.config.list_physical_devices('GPU') and execution_mode == 'GPU':\n      limit_memory_growth()\n      strategy = tf.distribute.MirroredStrategy()\n      enable_mixed_precision() # Enable mixed precision policy\n      print(\"Running on GPU\")\n\n    else:\n      strategy = tf.distribute.get_strategy()\n      print(\"Running on CPU\")","metadata":{"id":"_naYJoLTdEID","outputId":"0b6da021-6eda-48f9-913f-e4ff4e13d564","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Librairies import","metadata":{"id":"HrpgCsE546QX"}},{"cell_type":"code","source":"# Core Libraries\nimport os\nimport gc\nimport re\nimport h5py\nimport time\nimport random\nimport copy\nimport psutil\nimport hashlib\nimport traceback\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom collections import defaultdict\nfrom multiprocessing import Pool\nfrom concurrent.futures import ThreadPoolExecutor\n\n# TensorFlow and Keras Imports\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers, Model, Input, regularizers, optimizers, callbacks\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.utils import register_keras_serializable\n\n# TensorFlow Add-ons\nimport tensorflow_addons as tfa\n\n# Keras Tuner for Hyperparameter Optimization\nimport keras_tuner as kt\nfrom keras_tuner import HyperModel\nfrom keras_tuner.tuners import RandomSearch\n\n# TensorFlow Pretrained Models\nfrom tensorflow.keras.applications import (\n    EfficientNetB0, EfficientNetB4, EfficientNetB7,\n    DenseNet121, DenseNet169, DenseNet201,\n    InceptionV3, InceptionResNetV2,\n    ResNet50, ResNet101, ResNet152V2,\n    VGG16, VGG19, Xception\n)\n\n# Data Oversampling\nfrom imblearn.over_sampling import SMOTE\n\n# Scikit-Learn Utilities\nfrom sklearn.metrics import (\n    roc_curve, roc_auc_score, auc,\n    accuracy_score, classification_report, confusion_matrix\n)\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Memory Profiling\nfrom memory_profiler import memory_usage\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","metadata":{"id":"xVoeHCS02e2Y","outputId":"6d386b77-58c4-416c-a45b-00a070641588","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Database setting","metadata":{"id":"FsYks2fL5YFs"}},{"cell_type":"code","source":"if download_refined_training_datasets:\n    if database == 'local':\n      %pip install -q dagshub\n      import dagshub.colab\n      DAGSHUB_REPO = dagshub.colab.login()\n    elif database == 'google_drive':\n      from google.colab import drive\n      drive.mount('/content/drive')\n      ROOT_DB_DIR='/content/drive/MyDrive/documents_travail'\n    else:\n      %pip install -q dagshub\n      import dagshub.colab\n      DAGSHUB_REPO = dagshub.colab.login()\n      mount_path = dagshub.storage.mount(DAGSHUB_REPO, cache=True)\n      print(f'Mount path: {mount_path}')\n      ROOT_DB_DIR=f'/content/{mount_path}'","metadata":{"id":"YP8fe04oqHI1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training variables","metadata":{"id":"HdkcJ8R-gQZ4"}},{"cell_type":"code","source":"# Define root directories based on the execution environment\nif execution_env == 'mac': \n    ROOT_DB_DIR = '/Users/amadou/local/datalab'\nelif execution_env == 'kaggle': \n    ROOT_DB_DIR = '/kaggle/'\nelse:\n    ROOT_DB_DIR = '/content'\n    \ninput_dir = f'{ROOT_DB_DIR}/input'\noutput_dir = f'{ROOT_DB_DIR}/working'\ncache_dir = f'{output_dir}/tf_cache'  # Temporary cache directory for TensorFlow\nproject_dir = f\"{output_dir}/isic_2024\"  # Project-specific directory\ntmp_dir = f'{output_dir}/tmp'  # Temporary files directory\nmodels_dir = f'{output_dir}/models'  # Directory to save model files\npretrained_models_dir = f'{input_dir}/pretrained/tensorflow2/default/1/'\n\n# Define dataset directories\ndatasets_dir = f'{input_dir}/datasets'  # Root dataset directory\ntraining_dir = f'{datasets_dir}/training'  # Training data directory\ntraining_metadata_dir = f'{training_dir}/metadata'  # Metadata for training data\n\n# Define directories for training data (positives and negatives)\ntrain_dir = f'{datasets_dir}/training/train'\ntrain_dir_pos = f'{train_dir}/positives'  # Positive training samples\ntrain_dir_negs = f'{train_dir}/negatives'  # Negative training samples\n\n# Define validation and evaluation data directories (positives and negatives)\nval_dir = f'{datasets_dir}/training/val'\nval_dir_pos = f'{val_dir}/positives'  # Positive validation samples\nval_dir_negs = f'{val_dir}/negatives'  # Negative validation samples\n\neval_dir = f'{datasets_dir}/training/eval'\neval_dir_pos = f'{eval_dir}/positives'  # Positive evaluation samples\neval_dir_negs = f'{eval_dir}/negatives'  # Negative evaluation samples\n\n# Define paths for inference\ninference_dir = f'{project_dir}/inference'  # Inference-related directory\ninference_model_path = os.path.join(models_dir, 'inference_model.h5')  # Model for inference\ninference_images_dir = f'{inference_dir}/images/hdf5'  # Directory to source inference images\ninference_images_path = f'{inference_images_dir}/test-image.hdf5'  # HDF5 file with test images\ninference_metadata_dir = f'{inference_dir}/metadata'  # Directory for inference metadata\ninference_tabular_data_path = os.path.join(inference_metadata_dir, 'test-metadata.csv')  # Test metadata for tabular data\npreprocessor_path = f'{inference_dir}/preprocessor_{run_id}.pkl'  # Preprocessor for inference\n\n# Define TFRecord directories\ntf_records_dir = f'{input_dir}/tf_records/{dataset_size}' if experimentation_mode == 'test' else f'{input_dir}/sample-20k/sample_20k_kaggle'\n\ntf_records_train_val_dir = f'{tf_records_dir}/train'  # TFRecords for training\ntf_records_val_dir = f'{tf_records_dir}/validation'  # TFRecords for validation\ntf_records_eval_dir = f'{tf_records_dir}/evaluation'  # TFRecords for evaluation\noversample_persisted_data = False  # Oversample persisted data when creating tf_records\n\n# Image and model settings\nimg_height = 148  # Height of the input images\nimg_width = 148  # Width of the input images\nimg_channels = 3  # Number of color channels\nimg_shape = (img_height, img_width, img_channels)  # Shape of the input images (H x W x C)\nimage_size = img_height  # Image size, set to height for consistency\n\n# Training and evaluation settings\nuse_cross_validation = False  # Whether to use cross-validation\naugment_train_data = True  # Augment training data\nuse_tabular_data = False  # Whether to include tabular data in the model\nforce_cache = True  # Force caching of data\ncache_in_memory = True if execution_env == 'colab' else False  # Cache in memory only for Colab\ndo_fine_tuning = True  # Whether to fine-tune the model\noversample_minority_class = True  # Oversample the minority class in imbalanced datasets\ninitial_epochs = 1 if experimentation_mode == 'test' else 5  # Initial epochs for training\nfine_tune_epochs = 1 if experimentation_mode == 'test' else 80  # Epochs for fine-tuning\nfreeze_base_model = False  # Whether to freeze the base model during training\n\n# Dataset buffering and shuffling\nbuffer_size = 1000 if execution_env == 'colab' else 700  # Buffer size for data shuffling\nshuffle_train_val_at_each_call = True  # Shuffle training/validation data at each call\ntrain_file_pattern = f'{tf_records_train_val_dir}/*.tfrecord'  # Pattern to match TFRecord files\n\n# Model and training configurations\nmodel_name = 'res_net_50'  # Name of the model to be used (ResNet50)\ntrain_individuals = False  # Whether to train individual models of the pretrained ensemble\nrun_evaluations = False\n\n# Batch size settings depending on environment and mode\nif experimentation_mode == 'test':\n    batch_size = 16 if execution_env == 'colab' else 16\n    val_batch_size = 16 if execution_env == 'colab' else 32\n    eval_batch_size = 4 if execution_env == 'colab' else 32\nelse:\n    batch_size = 128 if execution_env == 'colab' else 128  # Batch size varies between Colab and local\n    val_batch_size = 128 if execution_env == 'colab' else 128  # Validation batch size\n    eval_batch_size = 128 if execution_env == 'colab' else 128  # Evaluation batch size\n\n# Model creation settings\ndropout_rate = 0.5\nl2_lambda = 0.02\nnum_tabular_features = 14\nkernel_initializer = 'he_normal'\nactivation = 'swish'","metadata":{"id":"7EJe_orxlYdY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if create_new_training_datasets:\n    !rm -rf {tf_records_dir}\n    \n!mkdir -p {tmp_dir}\n!mkdir -p {sample_dir}\n!mkdir -p {models_dir}\n!mkdir -p {tf_records_train_val_dir}\n!mkdir -p {tf_records_val_dir}\n!mkdir -p {tf_records_eval_dir}\n!mkdir -p {inference_dir}\n!mkdir -p {inference_images_dir}\n!mkdir -p {inference_metadata_dir}","metadata":{"id":"cHzmyIyutgZI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{"id":"EX8uhVi3g--9"}},{"cell_type":"markdown","source":"## Preparation utils","metadata":{"id":"QtvwrF9uozL3"}},{"cell_type":"markdown","source":"### Formatting utils","metadata":{"id":"rumQlSCcCHpX"}},{"cell_type":"code","source":"# Format all the metadatas to have the same structure\ndef format_tabular_data(tabular_data_paths):\n    tabular_dfs = []\n    for path in tabular_data_paths:\n        file_name = os.path.basename(path)\n        if file_name == 'train_metadata_2024.csv' or file_name == 'test-metadata.csv':\n            metadata_2024 = pd.read_csv(path, low_memory=False)\n            metadata_2024.fillna({'age_approx': metadata_2024.age_approx.median(), 'sex': 'unknown', 'anatom_site_general': 'unknown'}, inplace=True)\n            tabular_dfs.append(metadata_2024[common_columns])\n        elif file_name == 'train_metadata_2020.csv':\n            metadata_2020 = pd.read_csv(path, low_memory=False).rename(columns=dict(zip(selected_2019_2020_columns, common_columns)))\n            metadata_2020.fillna({'age_approx': metadata_2020.age_approx.median(), 'sex': 'unknown', 'anatom_site_general': 'unknown'}, inplace=True)\n            tabular_dfs.append(metadata_2020[common_columns])\n        elif file_name == 'train_metadata_2019.csv':\n            metadata_2019 = pd.read_csv(path, low_memory=False).rename(columns=dict(zip(selected_2019_2020_columns, common_columns)))\n            metadata_2019.fillna({'age_approx': metadata_2019.age_approx.median(), 'anatom_site_general': 'unknown'}, inplace=True)\n            tabular_dfs.append(metadata_2019[common_columns])\n    return tabular_dfs\n\n\n# Format and encode tabular data\ndef format_and_encode_tabular_data(tabular_data_paths, train_images, preprocessor=None):\n    tf.print('Formatting and encoding tabular data...')\n    tabular_data = pd.concat(format_tabular_data(tabular_data_paths), axis=0)\n\n    # Reindex tabular data to align with image IDs\n    train_tabular = tabular_data.set_index('isic_id').reindex(train_images).reset_index(drop=True)\n\n    if preprocessor is None:\n        # Create the preprocessor if not provided (for training)\n        print('Creating a new preprocessor to encode tabular data')\n        preprocessor = fit_tabular_encoder(train_tabular)\n        save_preprocessor(preprocessor, preprocessor_path)\n        encoded_tabular_data = encode_tabular_data(train_tabular, preprocessor)\n    else:\n        print('Using provided preprocessor to encode tabular data')\n        encoded_tabular_data = encode_tabular_data(train_tabular, preprocessor)\n\n    return encoded_tabular_data","metadata":{"id":"U0_SQA_GCMRt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding utils","metadata":{"id":"YkHXdiUvCP5x"}},{"cell_type":"code","source":"# Fit the encoder for tabular data using dense format\ndef fit_tabular_encoder(tabular_data):\n    num_cols = ['age_approx']\n    one_hot_cols = ['anatom_site_general', 'sex']\n\n    # Using dense format for OneHotEncoder\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), num_cols),\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), one_hot_cols)  # Dense output\n        ]\n    )\n    preprocessor.fit(tabular_data)\n    return preprocessor\n\n# Encode tabular data with dense format, called only after fit_tabular_encoder\n# or directely for val and eval data\ndef encode_tabular_data(tabular_data, preprocessor):\n    # Use the preprocessor to transform the tabular data into dense format\n    return preprocessor.transform(tabular_data)\n\n# Save and load preprocessor for reuse between training and inference\ndef save_preprocessor(preprocessor, file_path):\n    with open(file_path, 'wb') as f:\n        pickle.dump(preprocessor, f)\n    print('Saved preprocessor at', file_path)\n\ndef load_preprocessor(file_path):\n    with open(file_path, 'rb') as f:\n        preprocessor = pickle.load(f)\n    print('Loaded preprocessor from', file_path)\n    return preprocessor\n","metadata":{"id":"7UTcn1mnCTTe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oversampling utils","metadata":{"id":"cHEP-sikCmTm"}},{"cell_type":"code","source":"# Optionally apply smote on the whole dataset to balance minority and majority\ndef smote_oversampling(image_paths, labels, tabular_data):\n    \"\"\"Apply SMOTE to the labels and tabular data, with synchronized image resampling.\"\"\"\n    tf.print(\"Applying SMOTE\")\n\n    # Separate image paths, labels, and tabular data by class\n    class_0_indices = [i for i, label in enumerate(labels) if label == 0]\n    class_1_indices = [i for i, label in enumerate(labels) if label == 1]\n\n    class_0_image_paths = [image_paths[i] for i in class_0_indices]\n    class_1_image_paths = [image_paths[i] for i in class_1_indices]\n\n    if tabular_data is not None:\n        class_0_tabular = tabular_data.iloc[class_0_indices].reset_index(drop=True)\n        class_1_tabular = tabular_data.iloc[class_1_indices].reset_index(drop=True)\n    else:\n        class_0_tabular = class_1_tabular = None\n\n    # Apply SMOTE to the labels and tabular data\n    smote = SMOTE(sampling_strategy='minority')\n\n    if tabular_data is not None:\n        oversampled_tabular_data, oversampled_labels = smote.fit_resample(tabular_data, labels)\n    else:\n        dummy_data = np.zeros((len(labels), 1))\n        _, oversampled_labels = smote.fit_resample(dummy_data, labels)\n        oversampled_tabular_data = None\n\n    # Prepare for synchronized resampling of image paths and tabular data\n    synthetic_image_paths = []\n    synthetic_tabular_data = []\n\n    class_0_count = len(class_0_image_paths)\n    class_1_count = len(class_1_image_paths)\n\n    for idx, label in enumerate(oversampled_labels):\n        if label == 0:\n            synthetic_image_paths.append(class_0_image_paths[idx % class_0_count])\n            if tabular_data is not None:\n                synthetic_tabular_data.append(class_0_tabular.iloc[idx % class_0_count].values)\n        else:\n            synthetic_image_paths.append(class_1_image_paths[idx % class_1_count])\n            if tabular_data is not None:\n                synthetic_tabular_data.append(class_1_tabular.iloc[idx % class_1_count].values)\n\n    # Ensure coherence by converting synthetic_tabular_data to DataFrame if needed\n    if tabular_data is not None:\n        synthetic_tabular_data = pd.DataFrame(synthetic_tabular_data, columns=tabular_data.columns)\n    else:\n        synthetic_tabular_data = None\n\n    # Return synchronized image paths, labels, and tabular data\n    return synthetic_image_paths, oversampled_labels, synthetic_tabular_data","metadata":{"id":"HD9kzgYPCqqQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Subset sampling utils","metadata":{"id":"s5UM7BRuCvdr"}},{"cell_type":"code","source":"def sample_data(image_paths, labels, tabular_data, sampling_fraction):\n    \"\"\"Sample a subset of data for test purposes.\"\"\"\n    tf.print(\"Sampling data...\")\n    sampled_indices = np.random.choice(len(image_paths), int(len(image_paths) * sampling_fraction), replace=False)\n    image_paths = [image_paths[i] for i in sampled_indices]\n    labels = [labels[i] for i in sampled_indices]\n    if tabular_data is not None:\n        tabular_data = tabular_data[sampled_indices]\n    return image_paths, labels, tabular_data","metadata":{"id":"1PG6BIF5C6c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Serialization utils","metadata":{"id":"skNdprmGfRQ8"}},{"cell_type":"code","source":"# Data writing to tf_records for later reuse if working in cloud\ndef write_dataset_to_tfrecords(image_paths, labels, tabular_data, output_prefix, num_workers, data_type='train'):\n    tf.print(f\"\\nStart writting {data_type} tfrecords...\")\n    chunk_size = len(image_paths) // num_workers\n    tf.print(f\"Number of workers: {num_workers}\")\n    tf.print(f\"Dataset size: {len(image_paths)}, chunk size: {chunk_size}\")\n\n    args = [\n        (\n            image_paths[i:i + chunk_size],\n            labels[i:i + chunk_size],\n            tabular_data[i:i + chunk_size] if tabular_data is not None else None,\n            f'{output_prefix}_{i}.tfrecord'\n        )\n        for i in range(0, len(image_paths), chunk_size)\n    ]\n\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        executor.map(write_tfrecord_single_batch, args)\n\ndef write_tfrecord_single_batch(args):\n    \"\"\"\n    Write a batch of data to a TFRecord file. This function processes images and optional tabular data,\n    serializes them into a TFRecord format, and saves the result to a file.\n\n    Args:\n        args: A tuple or list containing the following:\n            - image_paths: List of file paths to the images (batch).\n            - labels: List of corresponding labels for the images (batch).\n            - tabular_data: Optional tabular data associated with the images (batch) (could be None).\n            - output_file: Path to the output TFRecord file.\n    \"\"\"\n    try:\n        image_paths, labels, tabular_data, output_file = args\n        # Print which file is being written\n        tf.print(f\"Writing TFRecords to: {output_file}\")\n\n        # Specify compression options for the TFRecord (GZIP in this case)\n        options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n\n        # Open a TFRecordWriter for writing data to the specified output file\n        with tf.io.TFRecordWriter(output_file, options=options) as writer:\n            # Loop through each image and label in the batch\n            for i, (img_path, label) in enumerate(zip(image_paths, labels)):\n                img_bytes = process_image(img_path)\n\n                # Convert the raw image into a feature that can be stored in the TFRecord\n                img_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_bytes]))\n\n                # Convert the label into a feature (using int64 since it's a categorical label)\n                label_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n\n                # Create a dictionary to store the image and label as features\n                features = {'image': img_feature, 'label': label_feature}\n\n                # If tabular data is used and provided, add it to the features dictionary\n                if tabular_data is not None:\n                    # Convert the tabular data to a float list feature for this specific sample\n                    tab_feature = tf.train.Feature(float_list=tf.train.FloatList(value=tabular_data[i]))\n                    features['tabular_data'] = tab_feature\n\n                # Create a TFRecord example from the features dictionary\n                example = tf.train.Example(features=tf.train.Features(feature=features))\n\n                # Serialize the example to a string and write it to the TFRecord file\n                writer.write(example.SerializeToString())\n    except Exception as e:\n        print(f\"Error in write_tfrecord: {e}\")\n        traceback.print_exc()\n\ndef process_image(img_path):\n    \"\"\"Helper function to read and process a single image.\"\"\"\n    try:\n        img_bytes = tf.io.read_file(img_path)\n        return img_bytes.numpy()  # Return the image bytes\n    except Exception as e:\n        raise Exception(f\"Error reading image {img_path}: {e}\")","metadata":{"id":"UYpXfYlJfPz1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data download utils","metadata":{"id":"LmnCG-oYNLuN"}},{"cell_type":"code","source":"def download_folder_from_dagshub(boto_client, bucket_name, remote_folder, local_folder):\n    \"\"\"\n    Downloads all files from a specified remote folder in DagsHub to a local folder.\n\n    Args:\n        boto_client: The boto client used to interact with DagsHub (S3-compatible).\n        bucket_name: Name of the DagsHub repository (S3 bucket).\n        remote_folder: The path to the folder in DagsHub (e.g., \"kaggle/isic-2024/datasets/tf_records/sample_1k/validation/\").\n        local_folder: The path to the local folder where files will be downloaded.\n    \"\"\"\n\n    # Ensure local folder exists\n    os.makedirs(local_folder, exist_ok=True)\n\n    # List all files in the remote folder\n    response = boto_client.list_objects_v2(Bucket=bucket_name, Prefix=remote_folder)\n\n    # Check if there are contents in the folder\n    if 'Contents' in response:\n        for obj in response['Contents']:\n            remote_file_path = obj['Key']\n            local_file_path = os.path.join(local_folder, os.path.basename(remote_file_path))\n\n            # Download each file\n            print(f\"Downloading {remote_file_path} to {local_file_path}...\")\n            boto_client.download_file(\n                Bucket=bucket_name,\n                Key=remote_file_path,\n                Filename=local_file_path\n            )\n        print(\"Download completed!\")\n    else:\n        print(f\"No files found in the folder: {remote_folder}\")\n\ndef resolve_remote_folder(local_folder):\n    return local_folder.replace(f'{ROOT_DB_DIR}/', '')","metadata":{"id":"CcazDGZVh__P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparation pipeline","metadata":{"id":"yu_Pj-0DDqju"}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder\nimport pickle\nfrom scipy.sparse import vstack, csr_matrix\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\ncommon_columns = ['isic_id', 'age_approx', 'sex', 'anatom_site_general']\nselected_2019_2020_columns = ['image_name', 'age_approx', 'sex', 'anatom_site_general_challenge']\n\n# Preparing data directories using dense tabular data format\ndef prepare_data_directories(train_dir, tabular_data_paths, sampling_fraction=1.0, use_oversampling=False, data_type='train'):\n    \"\"\"Load image paths, labels, and optionally tabular data\"\"\"\n\n    tf.print(f\"\\nStart {data_type} data preparation...\")\n\n    train_image_paths = glob(os.path.join(train_dir, '**', '*.jpg'), recursive=True)\n    train_labels = [1 if os.path.dirname(path).endswith('positives') else 0 for path in train_image_paths]\n    train_images = [os.path.basename(path).split('.')[0] for path in train_image_paths]\n\n    data_type = data_type.capitalize()\n    tf.print(f\"{data_type} labels length before preparing data\", len(train_labels))\n\n    if use_tabular_data:\n        # Load the preprocessor for validation or fit it for training data\n        preprocessor = None if data_type == 'Train' else load_preprocessor(preprocessor_path)\n\n        train_tabular = format_and_encode_tabular_data(tabular_data_paths, train_images, preprocessor=preprocessor)\n    else:\n        train_tabular = None\n\n    # oversample only train data\n    if use_oversampling:\n        train_image_paths, train_labels, train_tabular = smote_oversampling(\n            train_image_paths, train_labels, train_tabular\n        )\n        tf.print(f\"{data_type} data size after oversampling:\", len(train_labels))\n\n    # sample all the datasets if sampling is needed\n    if sampling_fraction < 1.0:\n        train_image_paths, train_labels, train_tabular = sample_data(\n            train_image_paths, train_labels, train_tabular, sampling_fraction\n        )\n        tf.print(f\"{data_type} data size after subsampling:\", len(train_labels))\n\n    if train_tabular is not None:\n        tabular_data_length = train_tabular.shape[0]\n\n        print(f\"Image data size: {len(train_image_paths)}\")\n        print(f\"Tabular data size: {tabular_data_length}\")\n\n        # Ensure that the number of tabular rows matches the number of image paths\n        assert len(train_image_paths) == tabular_data_length, \"Image paths and tabular data do not match\"\n    return (train_image_paths, train_labels, train_tabular)","metadata":{"id":"pgdFVV6vDsBs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run","metadata":{"id":"NQN-Ofv4fr0n"}},{"cell_type":"markdown","source":"### Download raw datasets","metadata":{"id":"-dRINe0fg1x1"}},{"cell_type":"code","source":"if download_raw_datasets:\n    boto_client = get_repo_bucket_client(\"AmadouMamane/dagshub-drive\", flavor=\"boto\")\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(eval_dir_pos), eval_dir_pos)\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(eval_dir_negs), eval_dir_negs)\n\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(val_dir_pos), val_dir_pos)\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(val_dir_negs), val_dir_negs)\n\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(train_dir_pos), train_dir_pos)\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(train_dir_negs), train_dir_negs)","metadata":{"id":"tA-VuKdAg2PP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download refined datasets","metadata":{"id":"7K1xS2pRhI9f"}},{"cell_type":"code","source":"if download_refined_training_datasets:\n    boto_client = get_repo_bucket_client(\"AmadouMamane/dagshub-drive\", flavor=\"boto\")\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(tf_records_eval_dir), tf_records_eval_dir)\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(tf_records_val_dir), tf_records_val_dir)\n    download_folder_from_dagshub(boto_client, \"dagshub-drive\", resolve_remote_folder(tf_records_train_val_dir), tf_records_train_val_dir)","metadata":{"id":"PsDlncmLhORm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preparation","metadata":{"id":"LjlJGFlRf_TT"}},{"cell_type":"code","source":"# Prepare and process data\nnum_workers = 10\n#total_samples = #49403 + 12100 + 4389 =65892\n\nif create_new_training_datasets:\n    sampling_fraction = dataset_size / 65892\n    metadata_2024_path = os.path.join(f'{training_metadata_dir}', 'train_metadata_2024.csv')\n    metadata_2020_path = os.path.join(f'{training_metadata_dir}', 'train_metadata_2020.csv')\n    metadata_2019_path = os.path.join(f'{training_metadata_dir}', 'train_metadata_2019.csv')\n    tabular_data_paths = [metadata_2024_path, metadata_2020_path, metadata_2019_path]\n\n    (train_image_paths, train_labels, train_tabular) = prepare_data_directories(train_dir, tabular_data_paths, sampling_fraction=sampling_fraction, use_oversampling=oversample_persisted_data, data_type='train')\n    (eval_image_paths, eval_labels, eval_tabular) = prepare_data_directories(eval_dir, tabular_data_paths, sampling_fraction=sampling_fraction, use_oversampling=oversample_persisted_data, data_type='eval')\n    (val_image_paths, val_labels, val_tabular) = prepare_data_directories(val_dir, tabular_data_paths, sampling_fraction=sampling_fraction, use_oversampling=oversample_persisted_data, data_type='Val')","metadata":{"id":"Ft7LSZOfp_gW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data serialization","metadata":{"id":"sLd3G8dfgFIL"}},{"cell_type":"code","source":"if create_new_training_datasets:\n    write_dataset_to_tfrecords(eval_image_paths, eval_labels, eval_tabular, f'{tf_records_eval_dir}/eval', num_workers=num_workers, data_type='train')\n    write_dataset_to_tfrecords(val_image_paths, val_labels, val_tabular, f'{tf_records_val_dir}/val', num_workers=num_workers, data_type='eval')\n    write_dataset_to_tfrecords(train_image_paths, train_labels, train_tabular, f'{tf_records_train_val_dir}/train', num_workers=num_workers, data_type='Val')\n    print('Number of train and val positives class samples', train_labels.count(1))\n    print('Number of val positives class samples', val_labels.count(1))\n    print('Number of eval positives class samples', eval_labels.count(1))","metadata":{"id":"ZG140Mukf3iu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data augmentation","metadata":{"id":"X-zFAXEutWTT"}},{"cell_type":"markdown","source":"#### Random hue saturation","metadata":{"id":"jfYraWRv1LNN"}},{"cell_type":"code","source":"sat_max_delta_hs=0.0005\nhue_max_delta_hs=0.0005\nval_max_delta_hs=0.0005\n@tf.function\ndef random_hue_saturation(image, hue_max_delta=hue_max_delta_hs, sat_max_delta=sat_max_delta_hs, val_max_delta=val_max_delta_hs, probability=0.5, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    def apply_hue_sat_val(img):\n        img = tf.image.random_hue(img, max_delta=hue_max_delta)\n        img = tf.image.random_saturation(img, lower=1 - sat_max_delta, upper=1 + sat_max_delta)\n        img = tf.image.random_brightness(img, max_delta=val_max_delta)\n        img = tf.clip_by_value(img, 0.0, 1.0)\n        return img\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_hue_sat_val(image),\n        lambda: image\n    )","metadata":{"id":"iDrV3NQ81INK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random shift scale rotate","metadata":{"id":"ASS4LwAj0jl_"}},{"cell_type":"code","source":"@tf.function\ndef random_shift_scale_rotate(image, shift_limit=0.05, scale_limit=0.05, rotate_limit=5, threshold=0.5, image_size=image_size, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    def apply_shift_scale_rotate(img, seed):\n\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=4)\n        angle_seed = seeds[0]\n        scale_seed = seeds[1]\n        dx_seed = seeds[3]\n        dy_seed = seeds[4]\n\n        # Random rotation\n        angle = tf.random.stateless_uniform([], -rotate_limit, rotate_limit, seed=angle_seed) * 3.14159265 / 180\n        img = tf.image.rot90(img, k=tf.cast(angle / (3.14159265 / 2), tf.int32))\n\n        # Random scaling\n        scale = tf.random.stateless_uniform([], 1 - scale_limit, 1 + scale_limit, seed=scale_seed)\n        new_size = tf.cast(tf.cast(tf.shape(img)[0:2], tf.float32) * scale, tf.int32)\n        img = tf.image.resize(img, new_size)\n\n        # Random shifting\n        max_dx = tf.cast(shift_limit * tf.cast(tf.shape(img)[1], tf.float32), tf.int32)\n        max_dy = tf.cast(shift_limit * tf.cast(tf.shape(img)[0], tf.float32), tf.int32)\n\n\n        dx = tf.random.stateless_uniform([], -max_dx, max_dx, dtype=tf.int32, seed=dx_seed)\n        dy = tf.random.stateless_uniform([], -max_dy, max_dy, dtype=tf.int32, seed=dy_seed)\n\n        target_height = tf.minimum(image_size, new_size[0] - tf.abs(dy))\n        target_width = tf.minimum(image_size, new_size[1] - tf.abs(dx))\n\n        img = tf.image.crop_to_bounding_box(\n            img,\n            offset_height=tf.maximum(0, dy),\n            offset_width=tf.maximum(0, dx),\n            target_height=target_height,\n            target_width=target_width\n        )\n\n        # Resize back to original size\n        img = tf.image.resize_with_crop_or_pad(img, target_height=image_size, target_width=image_size)\n        return img\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: apply_shift_scale_rotate(image, seed),\n        lambda: image\n    )","metadata":{"id":"fM1u4CFb0kv6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random crop","metadata":{"id":"YRlY_69rQdrF"}},{"cell_type":"code","source":"@tf.function\ndef random_crop(image, min_crop_size_ratio=0.7, max_crop_size_ratio=1.0, probability=0.5, seed=None):\n    def apply_random_crop_per_image(img):\n        img_shape = tf.shape(img)\n        height = img_shape[0]\n        width = img_shape[1]\n\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=4)\n        crop_seed = seeds[0]\n        size_seed = seeds[1]\n        y_seed = seeds[2]\n        x_seed = seeds[3]\n\n        # Randomly determine the crop size ratio within the given range using the seed\n        crop_size_ratio = tf.random.stateless_uniform([], minval=min_crop_size_ratio, maxval=max_crop_size_ratio, seed=size_seed)\n\n        # Calculate the crop size based on the randomly chosen ratio\n        crop_height = tf.cast(crop_size_ratio * tf.cast(height, tf.float32), tf.int32)\n        crop_width = tf.cast(crop_size_ratio * tf.cast(width, tf.float32), tf.int32)\n\n        y_maxval = tf.maximum(height - crop_height, 1)  # Ensure maxval is > min_val\n        x_maxval = tf.maximum(width - crop_width, 1)\n\n        # Randomly select the top-left corner of the crop using the seed\n        y1 = tf.random.stateless_uniform([], minval=0, maxval=x_maxval, dtype=tf.int32, seed=y_seed)\n        x1 = tf.random.stateless_uniform([], minval=0, maxval=y_maxval, dtype=tf.int32, seed=x_seed)\n\n        # Define the bottom-right corner of the crop\n        y2 = y1 + crop_height\n        x2 = x1 + crop_width\n\n        # Crop the image\n        cropped_img = img[y1:y2, x1:x2, :]\n\n        # Set a static shape for cropped image if necessary before resizing (adjust as needed)\n        cropped_img.set_shape([None, None, img.shape[-1]])\n\n        # Resize the cropped image back to the original size\n        cropped_img = tf.image.resize(cropped_img, [height, width], method='bilinear')\n\n        return cropped_img\n\n              # Use stateless random to generate a seed if none is provided\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    # Apply random crop with the specified probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_random_crop_per_image(image),\n        lambda: image\n    )","metadata":{"id":"kXgy9gOU0GP9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random cutout","metadata":{"id":"d9jLIru7PnQA"}},{"cell_type":"code","source":"@tf.function\ndef random_cutout(image, max_cutout_size_ratio=0.3, probability=0.5, seed=None):\n    def apply_cutout(img):\n        img_shape = tf.shape(img)\n        height = img_shape[-3]\n        width = img_shape[-2]\n\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=3)\n        cutout_seed = seeds[0]\n        x_center_seed = seeds[1]\n        y_center_seed = seeds[2]\n\n        # Generate the cutout size using a stateless random function with a seed\n        cutout_size = tf.random.stateless_uniform([], minval=0, maxval=max_cutout_size_ratio, dtype=tf.float32, seed=cutout_seed) * tf.cast(tf.minimum(height, width), tf.float32)\n        cutout_size = tf.cast(cutout_size, tf.int32)\n\n       # Randomly select the center position of the cutout using the seed\n        x_center = tf.random.stateless_uniform([], minval=0, maxval=width, dtype=tf.int32, seed=x_center_seed)\n        y_center = tf.random.stateless_uniform([], minval=0, maxval=height, dtype=tf.int32, seed=y_center_seed)\n\n        # Define the top-left and bottom-right corners of the cutout\n        x1 = tf.clip_by_value(x_center - cutout_size // 2, 0, width)\n        y1 = tf.clip_by_value(y_center - cutout_size // 2, 0, height)\n        x2 = tf.clip_by_value(x_center + cutout_size // 2, 0, width)\n        y2 = tf.clip_by_value(y_center + cutout_size // 2, 0, height)\n\n        # Create the cutout mask\n        mask = tf.ones_like(img)\n        x_range = tf.range(x1, x2)\n        y_range = tf.range(y1, y2)\n\n        # Generate all the coordinates to update in the mask\n        y_grid, x_grid = tf.meshgrid(y_range, x_range)\n        coords = tf.stack([y_grid, x_grid], axis=-1)\n        coords = tf.reshape(coords, [-1, 2])\n\n        # Update the mask with zeros at the selected coordinates\n        mask = tf.tensor_scatter_nd_update(mask, coords, tf.zeros([tf.shape(coords)[0], img_shape[-1]], dtype=img.dtype))\n\n        # Apply the mask to the image\n        img = img * mask\n\n        return img\n\n    # Use stateless random to generate a seed if none is provided\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n    # Apply cutout with the specified probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_cutout(image),\n        lambda: image\n      )","metadata":{"id":"pp9e5uxKvh95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distorsions","metadata":{"id":"Rw74RPsYU-ZU"}},{"cell_type":"markdown","source":"#### Random elastic transform","metadata":{"id":"JWgZUXBdVDy3"}},{"cell_type":"code","source":"@tf.function\ndef random_elastic_transform(image, alpha_range=(1, 3), sigma_range=(0.8, 2), padding_size_range=(5, 15),  probability=0.5, seed=None):\n    # Use stateless random to generate a seed if none is provided\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    # Generate random values for alpha, sigma, and padding_size from their respective ranges\n    alpha = tf.random.stateless_uniform([], minval=alpha_range[0], maxval=alpha_range[1], seed=seed)\n    sigma = tf.random.stateless_uniform([], minval=sigma_range[0], maxval=sigma_range[1], seed=seed)\n    padding_size = tf.random.stateless_uniform([], minval=padding_size_range[0], maxval=padding_size_range[1], seed=seed, dtype=tf.int32)\n\n    def apply_elastic_transform(img):\n        img = tf.cast(img, tf.float32)\n        height, width, channels = tf.shape(img)[0], tf.shape(img)[1], tf.shape(img)[2]\n\n        # Pad the image to reduce boundary artifacts\n        img_padded = tf.pad(img, [[padding_size, padding_size], [padding_size, padding_size], [0, 0]], mode='REFLECT')\n\n        # Update dimensions after padding\n        padded_height, padded_width = tf.shape(img_padded)[0], tf.shape(img_padded)[1]\n\n        # Split seed for reproducibility\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n        seeds = tf.random.experimental.stateless_split(seed, num=2)\n        dx_seed = seeds[0]\n        dy_seed = seeds[1]\n\n        # Generate random displacement fields\n        dx = tf.random.stateless_normal([padded_height, padded_width], mean=0.0, stddev=sigma, seed=dx_seed)\n        dy = tf.random.stateless_normal([padded_height, padded_width], mean=0.0, stddev=sigma, seed=dy_seed)\n\n        # Smooth the displacement fields\n        kernel_size = 13\n        kernel = tf.ones((kernel_size, kernel_size, 1, 1)) / (kernel_size * kernel_size)\n        dx = tf.nn.depthwise_conv2d(tf.expand_dims(tf.expand_dims(dx, axis=-1), axis=0), kernel, strides=[1, 1, 1, 1], padding=\"SAME\")[0, ..., 0]\n        dy = tf.nn.depthwise_conv2d(tf.expand_dims(tf.expand_dims(dy, axis=-1), axis=0), kernel, strides=[1, 1, 1, 1], padding=\"SAME\")[0, ..., 0]\n\n        # Scale the displacement fields\n        dx *= alpha\n        dy *= alpha\n\n        # Generate meshgrid and add the displacements\n        xs, ys = tf.meshgrid(tf.range(padded_width), tf.range(padded_height))\n        xs = tf.cast(xs, tf.float32) + dx\n        ys = tf.cast(ys, tf.float32) + dy\n\n        # Clip the values to be within image dimensions\n        xs = tf.clip_by_value(xs, 0.0, tf.cast(padded_width - 1, tf.float32))\n        ys = tf.clip_by_value(ys, 0.0, tf.cast(padded_height - 1, tf.float32))\n\n        # Resample using the distorted coordinates\n        distorted_indices = tf.stack([ys, xs], axis=-1)\n        distorted_image_padded = tf.gather_nd(img_padded, tf.cast(distorted_indices, tf.int32))\n\n        # Remove padding by cropping back to the original image size\n        distorted_image = distorted_image_padded[padding_size:padding_size + height, padding_size:padding_size + width, :]\n\n        return distorted_image\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_elastic_transform(image),\n        lambda: image\n    )","metadata":{"id":"HrdEJ-Sck5ga","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random grid distortion","metadata":{"id":"fd8W-h6DVIvJ"}},{"cell_type":"code","source":"@tf.function\ndef random_grid_distortion(image, num_steps=10, distort_limit=0.05, probability=0.5, seed=None):\n             # Use stateless random to generate a seed if none is provided\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    def apply_grid_distortion(img):\n        img_shape = tf.shape(img)\n        height = tf.cast(img_shape[0], tf.float32)\n        width = tf.cast(img_shape[1], tf.float32)\n\n        # Define grid step size\n        x_step = tf.cast(width // num_steps, tf.float32)\n        y_step = tf.cast(height // num_steps, tf.float32)\n\n\n        # Verification for tf.random.stateless_uniform so that min_val and mx_val != 0\n        x_step = tf.cond(x_step > 0, lambda: x_step, lambda: tf.constant(1e-7))\n        y_step = tf.cond(y_step > 0, lambda: y_step, lambda: tf.constant(1e-7))\n\n\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=2)\n        x_offsets_seed = seeds[0]\n        y_offsets_seed = seeds[1]\n\n        # Generate random offsets\n        x_offsets = tf.random.stateless_uniform(\n            shape=[num_steps + 1, num_steps + 1],\n            minval=-distort_limit * x_step,\n            maxval=distort_limit * x_step,\n            dtype=tf.float32,\n            seed=x_offsets_seed\n        )\n        y_offsets = tf.random.stateless_uniform(\n            shape=[num_steps + 1, num_steps + 1],\n            minval=-distort_limit * y_step,\n            maxval=distort_limit * y_step,\n            dtype=tf.float32,\n            seed=y_offsets_seed\n        )\n\n        # Create grid of coordinates\n        x = tf.linspace(0.0, width, num_steps + 1)\n        y = tf.linspace(0.0, height, num_steps + 1)\n        x_t, y_t = tf.meshgrid(x, y)\n\n        # Apply offsets\n        x_t = x_t + x_offsets\n        y_t = y_t + y_offsets\n\n        # Interpolate to get dense flow field\n        x_interp = tf.image.resize(x_t[..., tf.newaxis], [height, width], method='bilinear')\n        y_interp = tf.image.resize(y_t[..., tf.newaxis], [height, width], method='bilinear')\n\n        # Stack and subtract identity grid\n        flow = tf.stack([y_interp[..., 0] - tf.range(height)[:, None],\n                         x_interp[..., 0] - tf.range(width)[None, :]], axis=-1)\n\n        # Apply flow to image\n        distorted_image = tfa.image.dense_image_warp(img[tf.newaxis, ...], flow[tf.newaxis, ...])[0]\n\n        return distorted_image\n\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_grid_distortion(image),\n        lambda: image\n    )\n","metadata":{"id":"BIbXupt0kncg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random optical distortion","metadata":{"id":"44p-jqlaskfR"}},{"cell_type":"code","source":"# Define distortion parameters\ndistort_limit_do = 0.0005\nshift_limit_do = 0.0005\n\n# Define augmentation layers outside the tf.function\nrandom_rotation = preprocessing.RandomRotation(factor=distort_limit_do)\nrandom_translation = preprocessing.RandomTranslation(height_factor=shift_limit_do,\n                                                     width_factor=shift_limit_do)\nrandom_zoom = preprocessing.RandomZoom(height_factor=(-distort_limit_do, distort_limit_do),\n                                       width_factor=(-distort_limit_do, distort_limit_do))\n\n@tf.function\ndef random_optical_distortion(image, probability=0.5, seed=None):\n    def apply_optical_distortion(img):\n        # Apply the augmentations\n        img = random_rotation(img, training=True)\n        img = random_translation(img, training=True)\n        img = random_zoom(img, training=True)\n        return img\n\n    # If seed is provided, split it for reproducible randomness\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_optical_distortion(image),\n        lambda: image\n    )","metadata":{"id":"_Is47HQHdcKM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random one of distorsion","metadata":{"id":"wN6HU68VvO1n"}},{"cell_type":"code","source":"distort_limit_do =0.0005 # defined oon function definition also\nshift_limit_do = 0.0005\n\nnum_steps_dg=10\ndistort_limit_dg=0.05\n\nalpha_range_de = (0.5,1.5)\nsigma_range_de =(0.5,1.0)\npadding_size_range_de = (5,10)\n\n@tf.function\ndef random_one_of_distortion(image, probability=0.7, seed=None):\n    # Generate seed if not provided\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    def apply_random_transform(img, seed):\n\n        # Generate a random choice for distortion transformation\n        choice = tf.random.stateless_uniform([], minval=0, maxval=2, dtype=tf.int32, seed=seed)\n        # Apply random distortions based on the choice\n        #img = tf.cond(choice == 0, lambda: random_optical_distortion(img, probability=1.0, seed=None), lambda: img)\n        img = tf.cond(choice == 0, lambda: random_grid_distortion(img, num_steps=num_steps_dg, distort_limit=distort_limit_dg, probability=1.0, seed=None), lambda: img)\n        img = tf.cond(choice == 1, lambda: random_elastic_transform(img, alpha_range=alpha_range_de, sigma_range=sigma_range_de, padding_size_range=padding_size_range_de, probability=1.0, seed=None), lambda: img)\n        return img\n\n    # Apply transformation based on the probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_random_transform(image, seed),\n        lambda: image\n    )","metadata":{"id":"rvA7WvSoj0n8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Blurings","metadata":{"id":"kxPVGK7c5ppq"}},{"cell_type":"code","source":"var_limit_gn=(0.0, 0.0001)\nsigma_range_gb=(0.0, 0.7)\nkernel_size_range_gb=(3, 4)\nangle_range_gm=(0, 30)\nkernel_size_range_gm=(1, 4)","metadata":{"id":"0dUBXJj8R4Nt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random gaussian blur","metadata":{"id":"C9vouzC4QuBp"}},{"cell_type":"code","source":"@tf.function\ndef random_gaussian_blur(image, kernel_size_range=(3, 7), sigma_range=(0.1, 2.0), probability=0.5, seed=None):\n    def gaussian_kernel(size: int, mean: float, std: float):\n        \"\"\"Creates a 2D Gaussian Kernel for convolution.\"\"\"\n        coords = tf.range(-(size // 2), size // 2 + 1, dtype=tf.float32)\n        g = tf.exp(-tf.pow(coords - mean, 2.0) / (2.0 * tf.pow(std, 2.0)))\n        g /= tf.reduce_sum(g)\n        gauss_kernel = tf.tensordot(g, g, axes=0)\n        return gauss_kernel / tf.reduce_sum(gauss_kernel)\n\n    def apply_blur(img):\n        # Generate a seed if none is provided for consistent randomness\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=2)\n        kernel_size_seed = seeds[0]\n        sigma_seed = seeds[1]\n\n\n        # Randomize the kernel size and sigma within the given ranges\n        random_kernel_size = tf.random.stateless_uniform([], minval=kernel_size_range[0], maxval=kernel_size_range[1], dtype=tf.int32, seed=kernel_size_seed)\n        random_kernel_size = tf.where(random_kernel_size % 2 == 0, random_kernel_size + 1, random_kernel_size)  # Ensure kernel size is odd\n        random_sigma = tf.random.stateless_uniform([], minval=sigma_range[0], maxval=sigma_range[1], seed=sigma_seed)\n\n        # Create the Gaussian kernel\n        kernel = gaussian_kernel(random_kernel_size, 0., random_sigma)\n        kernel = kernel[:, :, tf.newaxis, tf.newaxis]\n        kernel = tf.tile(kernel, [1, 1, 3, 1])  # Match kernel to the RGB channels\n\n        # Ensure image is 4D (batch size, height, width, channels)\n        img = tf.cond(tf.equal(tf.rank(img), 3),  # Check if the image is rank 3 (without batch dimension)\n                      lambda: tf.expand_dims(img, axis=0),  # Add batch dimension\n                      lambda: img)  # If already 4D, pass as-is\n\n        # Apply depthwise convolution (motion blur)\n        img = tf.nn.depthwise_conv2d(img, kernel, [1, 1, 1, 1], padding='SAME')[0]\n\n        return img\n\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_blur(image),\n        lambda: image\n    )","metadata":{"id":"7GsexQzMQr0D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random motion blur","metadata":{"id":"PwQ642z8SGOB"}},{"cell_type":"code","source":"@tf.function\ndef random_motion_blur(image, kernel_size_range=(3, 7), angle_range=(0, 360), probability=0.5, seed=None):\n    def motion_blur_kernel(img, kernel_size, angle):\n        center = tf.cast(kernel_size // 2, tf.float32)\n\n        # Calculate the angle in radians\n        angle_rad = angle * (3.14159265359 / 180.0)\n        cos_a = tf.cos(angle_rad)\n        sin_a = tf.sin(angle_rad)\n\n        # Create meshgrid for indices\n        x = tf.cast(tf.range(kernel_size), tf.float32) - center\n        y = tf.cast(tf.range(kernel_size), tf.float32) - center\n        xx, yy = tf.meshgrid(x, y)\n\n        # Compute the distance to simulate motion blur in the specified direction\n        distance = xx * cos_a + yy * sin_a\n        kernel = tf.cast(tf.abs(distance) < 0.5, tf.float32)\n\n        # Normalize the kernel\n        kernel = kernel / tf.reduce_sum(kernel)\n\n        # Expand dimensions to 4D for depthwise convolution\n        kernel = kernel[:, :, tf.newaxis, tf.newaxis]\n        kernel = tf.tile(kernel, [1, 1, tf.shape(img)[-1], 1])  # Match the number of channels\n\n        return kernel\n\n    def apply_motion_blur(img, seed):\n         # Generate a seed if none is provided for consistent randomness\n        if seed is None:\n            seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=2)\n        kernel_size_seed = seeds[0]\n        angle_seed = seeds[1]\n        # Randomly choose a kernel size and angle within the specified range\n        kernel_size = tf.random.stateless_uniform([], minval=kernel_size_range[0], maxval=kernel_size_range[1], dtype=tf.int32, seed=kernel_size_seed)\n        angle = tf.random.stateless_uniform([], minval=angle_range[0], maxval=angle_range[1], dtype=tf.float32, seed=angle_seed)\n\n        # Create the motion blur kernel\n        kernel = motion_blur_kernel(img, kernel_size, angle)\n\n        # Ensure image is 4D (batch size, height, width, channels)\n        img = tf.cond(tf.equal(tf.rank(img), 3),  # Check if the image is rank 3 (without batch dimension)\n                      lambda: tf.expand_dims(img, axis=0),  # Add batch dimension\n                      lambda: img)  # If already 4D, pass as-is\n\n        # Apply depthwise convolution (motion blur)\n        img = tf.nn.depthwise_conv2d(img, kernel, [1, 1, 1, 1], padding='SAME')[0]\n\n        return img\n\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    # Apply motion blur with a certain probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n                   lambda: apply_motion_blur(image, seed=None),\n                   lambda: image)\n","metadata":{"id":"KcDm-kmDSJsD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random median blur","metadata":{"id":"TVVVuQW6SX7m"}},{"cell_type":"code","source":"@tf.function\ndef random_median_blur(image, kernel_size=5, probability=0.5, seed=None):\n    def apply_median_blur(img, seed):\n            # If no seed is provided, generate one\n        if seed is None:\n            seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n        # Convert image to uint8 for applying the median filter\n        img = tf.cast(img * 255.0, tf.uint8)\n        img = tf.image.rgb_to_grayscale(img)\n        # Apply median blur\n        img = tf.image.median_filter2d(img, filter_shape=[kernel_size, kernel_size])\n        img = tf.image.grayscale_to_rgb(img)\n        # Convert back to float32 after blurring\n        return tf.cast(img, tf.float32) / 255.0\n\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    # Apply the blur based on the given probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_median_blur(image, seed=None),\n        lambda: image\n    )","metadata":{"id":"7Evz2HsPSbGW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Radom gaussian noise","metadata":{"id":"loASl3TOSe8I"}},{"cell_type":"code","source":"@tf.function\ndef random_gaussian_noise(image, var_limit=(0.01, 0.1), probability=0.5, seed=None):\n    def apply_noise(img, seed):\n          # Create a seed if none is provided to ensure consistent randomness\n        if seed is None:\n            seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        seeds = tf.random.experimental.stateless_split(seed, num=2)\n        stddev_seed = seeds[0]\n        noise_seed = seeds[1]\n        # Generate random standard deviation for the noise within the given limits\n        stddev = tf.random.stateless_uniform([], minval=var_limit[0], maxval=var_limit[1], seed=stddev_seed)\n\n        # Generate random Gaussian noise\n        noise = tf.random.stateless_normal(shape=tf.shape(img), mean=0.0, stddev=stddev, dtype=tf.float32, seed=noise_seed)\n\n        # Add noise to the image and clip values to ensure valid pixel values [0, 1]\n        return tf.clip_by_value(img + noise, 0.0, 1.0)\n\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n    # Apply noise with a certain probability\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_noise(image, seed=None),\n        lambda: image\n    )\n","metadata":{"id":"PrZq5YB0SihD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random one of blur","metadata":{"id":"xVvyyhmiSnxz"}},{"cell_type":"code","source":"@tf.function\ndef random_one_of_blur(image, probability=0.7, seed=None):\n    def apply_random_transform(img, seed):\n        # If no seed is provided, generate one\n        if seed is None:\n            seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n        # Generate a random choice using stateless randomness for reproducibility\n        choice = tf.random.stateless_uniform([], minval=0, maxval=3, dtype=tf.int32, seed=seed)\n\n        # Apply different blur/noise transformations based on random choice\n        img = tf.cond(choice == 0,\n                      lambda: random_motion_blur(img, kernel_size_range=kernel_size_range_gm, angle_range=angle_range_gm, probability=1.0, seed=None),\n                      lambda: img)\n\n        img = tf.cond(choice == 1,\n                      lambda: random_gaussian_blur(img, kernel_size_range=kernel_size_range_gb, sigma_range=sigma_range_gb, probability=1.0, seed=None),\n                      lambda: img)\n\n        img = tf.cond(choice == 2,\n                      lambda: random_gaussian_noise(img, var_limit=var_limit_gn, probability=1.0, seed=None),\n                      lambda: img)\n        return img\n\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n    # Use the main condition to determine whether to apply a transformation or not\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), probability),\n        lambda: apply_random_transform(image, seed=None),\n        lambda: image\n    )","metadata":{"id":"Ve6I6Ia4nlfE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef random_transpose(image, threshold=0.5, seed=None):\n    # Ensure stateless randomness for reproducibility\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: tf.image.transpose(image),\n        lambda: image\n    )\n\n\n@tf.function\ndef random_vertical_flip(image, threshold=0.5, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: tf.image.flip_up_down(image),\n        lambda: image\n    )\n\n@tf.function\ndef random_horizontal_flip(image, threshold=0.5, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: tf.image.flip_left_right(image),\n        lambda: image\n    )\n\n@tf.function\ndef random_brightness(image, max_delta=0.05, threshold=0.5, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: tf.image.random_brightness(image, max_delta=max_delta),\n        lambda: image\n    )\n\n@tf.function\ndef random_contrast(image, lower=0.9, upper=1.1, threshold=0.5, seed=None):\n    if seed is None:\n        seed = tf.random.uniform([2], maxval=2**31 - 1, dtype=tf.int32)\n\n    return tf.cond(\n        tf.less(tf.random.stateless_uniform([], minval=0, maxval=1, seed=seed), threshold),\n        lambda: tf.image.random_contrast(image, lower=lower, upper=upper),\n        lambda: image\n    )\n","metadata":{"id":"PQ7HFQsuizgh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply augmentations","metadata":{"id":"TKkCvkJHEvZW"}},{"cell_type":"code","source":"threshold_low=0.4\nthreshold_medium=0.5\nthreshold_high=0.7\nthreshold_very_high=0.6","metadata":{"id":"smgAKxV_7N0x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_low=0.4\nthreshold_medium=0.65\nthreshold_high=0.7\nthreshold_very_high=0.65\n\n@tf.function\ndef augment_minority(image, threshold=threshold_very_high, seed=None):\n    # Apply a series of augmentations with appropriate thresholds\n    image = random_transpose(image, threshold_high, seed=seed)\n    image = random_vertical_flip(image, threshold=threshold_high, seed=seed)\n    image = random_horizontal_flip(image, threshold=threshold_high, seed=seed)\n    image = random_brightness(image,  threshold=threshold, seed=seed)\n    image = random_contrast(image, threshold=threshold, seed=seed)\n\n    image = random_one_of_blur(image, probability=threshold_medium, seed=seed)\n    image = random_one_of_distortion(image, probability=threshold_medium, seed=seed)\n    image = random_hue_saturation(image, probability=threshold_medium, seed=seed)\n    image = random_crop(image, probability=threshold_medium, seed=seed)\n    image = random_cutout(image, probability=threshold_medium, seed=seed)\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n@tf.function\ndef augment_majority(image, threshold=threshold_low, seed=None):\n    # Apply augmentations with lower thresholds for majority class\n    image = random_transpose(image,  threshold=threshold, seed=seed)\n    image = random_vertical_flip(image,  threshold=threshold, seed=seed)\n    image = random_horizontal_flip(image,  threshold=threshold, seed=seed)\n    image = random_brightness(image, threshold=threshold, seed=seed)\n    image = random_contrast(image, threshold=threshold, seed=seed)\n\n    image = random_one_of_blur(image, probability=threshold, seed=seed)\n    image = random_one_of_distortion(image, probability=threshold, seed=seed)\n    image = random_hue_saturation(image, probability=threshold, seed=seed)\n    image = random_crop(image, probability=threshold, seed=seed)\n    image = random_cutout(image, probability=threshold, seed=seed)\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n@tf.function\ndef augment_image_batch_old(images, labels):\n    \"\"\"\n    This function applies augmentations to batches of images based on the label using vectorized operations.\n    It applies augment_minority if label == 1, otherwise augment_majority.\n    \"\"\"\n    # Define how to augment each image based on the label\n    def augment_single(image, label):\n        if label == 1:\n            return augment_minority(image)\n        else:\n            return augment_majority(image)\n\n    # Apply augmentations to the batch using tf.map_fn\n    augmented_images = tf.map_fn(lambda x: augment_single(x[0], x[1]), (images, labels), fn_output_signature=tf.float32)\n    # Assuming augment_single takes image and label as input and returns augmented image\n    return augmented_images\n\n@tf.function\ndef augment_image_batch(images, labels):\n    \"\"\"\n    Apply augmentations based on the label (minority or majority) using tf.where.\n    Labels of 1 apply minority augmentations, labels of 0 apply majority augmentations.\n    \"\"\"\n    # Create a mask where label == 1 (for minority)\n    mask = tf.equal(labels, 1)\n\n    # Apply augmentations conditionally based on the mask\n    augmented_images = tf.where(mask[:, None, None, None],\n                                #augment_minority(images),\n                                #augment_majority(images)\n                                tf.map_fn(lambda x: augment_minority(x), images, fn_output_signature=tf.float32),\n                                tf.map_fn(lambda x: augment_majority(x), images, fn_output_signature=tf.float32)\n                               )\n    return augmented_images\n\n@tf.function\ndef apply_augmentations(dataset, use_tabular_data=use_tabular_data):\n    tf.print(\"Started data augmentation...\")\n    if use_tabular_data:\n        # Map using (image, tabular_data, label)\n        return dataset.map(lambda img, tab, lbl: (augment_image_batch(img, lbl), tab, lbl),\n                           num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        # Map using only (image, label)\n        return dataset.map(lambda img, lbl: (augment_image_batch(img, lbl), lbl),\n                           num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"id":"O0b9_Cdp19nS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data processing","metadata":{"id":"IDupvFefwQyQ"}},{"cell_type":"markdown","source":"### Caching utils","metadata":{"id":"9Cf22aP_e2rr"}},{"cell_type":"code","source":"def get_file_modification_time(file_path):\n    \"\"\"Get the last modification time of a file.\"\"\"\n    return os.path.getmtime(file_path)\n\ndef get_file_checksum(file_path):\n    \"\"\"Generate a checksum for a file's contents.\"\"\"\n    md5_hash = hashlib.md5()\n    with tf.io.gfile.GFile(file_path, 'rb') as f:\n        while chunk := f.read(8192):\n            md5_hash.update(chunk)\n    return md5_hash.hexdigest()\n\ndef get_dataset_checksum(file_pattern, check_modification_time=False):\n    \"\"\"Generate a checksum for the dataset based on file contents or modification times.\"\"\"\n    files = tf.io.gfile.glob(file_pattern)\n    if not files:\n        raise FileNotFoundError(f\"No files match the pattern: {file_pattern}\")\n\n    combined_checksum = hashlib.md5()\n\n    for file in files:\n        if check_modification_time:\n            # Include file modification times\n            file_mod_time = str(get_file_modification_time(file))\n            combined_checksum.update(file_mod_time.encode('utf-8'))\n        else:\n            # Include file contents checksum\n            file_checksum = get_file_checksum(file)\n            combined_checksum.update(file_checksum.encode('utf-8'))\n\n    return combined_checksum.hexdigest()\n\ndef generate_cache_name(file_pattern, checksum, prefix=\"dataset_cache\", run_id=None):\n    \"\"\"Generate a unique cache name based on the file pattern and dataset checksum.\"\"\"\n    if file_pattern:\n        file_pattern_hash = hashlib.md5(file_pattern.encode('utf-8')).hexdigest()\n    else:\n        file_pattern_hash = \"not_provided\"\n\n    cache_name = f\"{prefix}_{file_pattern_hash}_checksum_{checksum}\"\n\n    if run_id:\n        cache_name += f\"_run_{run_id}\"\n\n    return cache_name\n\ndef remove_lockfile_if_exists(cache_path):\n    \"\"\"Remove a lockfile if it exists.\"\"\"\n    lockfile_path = f\"{cache_path}.lockfile\"\n    if os.path.exists(lockfile_path):\n        print(f\"Removing stale lockfile: {lockfile_path}\")\n        os.remove(lockfile_path)\n\ndef check_cache_exists(cache_dir, cache_name):\n    \"\"\"Check if the cache exists by looking for any cache files with the given prefix.\"\"\"\n    cache_pattern = os.path.join(cache_dir, cache_name + \"*\")\n    return len(glob(cache_pattern)) > 0\n\n# Force full caching of the dataset by fully consuming it\ndef fully_cache_dataset(dataset, train_val_dataset_steps):\n    \"\"\"Fully cache the dataset by iterating over all elements.\"\"\"\n    print(\"Triggering full dataset caching...\")\n    for _ in dataset:\n        pass  # Consume all elements to force caching\n    print(\"Caching complete.\")\n    return dataset\n\ndef persist_dataset(dataset, file_pattern, dataset_steps, cache_in_memory, cache_dir=cache_dir,\n                    prefix=\"dataset_cache\", run_id=None, check_modification_time=False, force_cache=True):\n    \"\"\"\n    Efficiently caches a TensorFlow dataset to memory or disk.\n\n    Args:\n        dataset (tf.data.Dataset): The TensorFlow dataset to cache.\n        file_pattern (str): Pattern for the dataset files.\n        dataset_steps (int): Number of steps in the dataset.\n        cache_in_memory (bool): Whether to cache the dataset in memory.\n        cache_dir (str): Directory for storing the cached dataset (if not in memory).\n        prefix (str): Prefix for cache file names.\n        run_id (str): Optional identifier to distinguish cache versions.\n        check_modification_time (bool): Consider file modification times for caching.\n        force_cache (bool): If True, forces cache re-generation.\n\n    Returns:\n        tf.data.Dataset: The cached dataset.\n    \"\"\"\n    # Generate the cache name and path\n    if file_pattern:\n        dataset_checksum = get_dataset_checksum(file_pattern, check_modification_time)\n    else:\n        dataset_checksum = 'not_provided'\n\n    cache_name = generate_cache_name(file_pattern, dataset_checksum, prefix, run_id)\n    cache_path = os.path.join(cache_dir, cache_name)\n\n    # If forcing cache or no cache exists, optionally clean the previous cache\n    if force_cache:\n        clean_cache_path(cache_path)\n        gc.collect()\n\n    # Use existing cache if available and not forced to recache\n    if not force_cache and check_cache_exists(cache_dir, cache_name):\n        print(f\"Using cached dataset at {cache_path}\")\n        #return dataset if cache_in_memory else dataset.cache(cache_path)\n        return dataset\n\n    # Create cache directory only if caching to disk\n    if not cache_in_memory:\n        os.makedirs(cache_dir, exist_ok=True)\n        print(f\"Caching dataset to disk at {cache_path}\")\n    else:\n        print(\"Caching dataset to memory\")\n\n    # Cache in memory or disk and ensure dataset is fully cached\n    dataset = dataset.cache() if cache_in_memory else dataset.cache(cache_path)\n    fully_cache_dataset(dataset, dataset_steps)\n\n    return dataset\n\ndef clean_cache_path(cache_path, cache_dir=cache_dir):\n    \"\"\"\n    Removes all lockfiles from the cache directory to clean up stale caches.\n    Args:\n        cache_dir (str): The directory where the cache is stored.\n    \"\"\"\n    try:\n        # List all files in the cache directory\n        for root, dirs, files in os.walk(cache_dir):\n            for file in files:\n                # Check if the file is a lockfile\n                if file.startswith(cache_path):\n                    lockfile_path = os.path.join(root, file)\n                    print(f\"Removing lockfile: {lockfile_path}\")\n                    os.remove(lockfile_path)\n        print(f\"Cache {cache_path} cleanup completed\")\n    except Exception as e:\n        print(f\"An error occurred while cleaning the cache path {cache_path}: {str(e)}\")","metadata":{"id":"RcFfFmxKe2rr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data processing utils","metadata":{"id":"h4j8TAaVe2rr"}},{"cell_type":"code","source":"def calculate_samples_per_class(labels):\n    unique, counts = np.unique(labels, return_counts=True)\n    return dict(zip(unique, counts))\n\ndef calculate_class_weights(samples_per_class):\n    total_samples = sum(samples_per_class.values())\n    return {cls: total_samples / (len(samples_per_class) * count) for cls, count in samples_per_class.items()}\n\ndef calculate_steps_per_epoch(total_samples, batch_size=batch_size, strategy=strategy):\n    steps = total_samples // (batch_size * strategy.num_replicas_in_sync)\n    if steps == 0:\n        raise Exception(f\"Calculated steps is 0 because the provided batch_size {batch_size} is too big !\\n Total samples is {total_samples}\")\n    return steps\n\ndef stratified_split_indices(labels, n_splits=5, shuffle=True, random_state=42):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n    return list(skf.split(np.zeros(len(labels)), labels))\n\ndef count_samples_in_tfrecord(file_pattern):\n    \"\"\"Count the total number of samples in the TFRecord files.\"\"\"\n    print(\"Calculating tfrecords dataset size...\")\n    dataset = tf.data.Dataset.list_files(file_pattern)\n    total_count = 0\n    for file in dataset:\n        count = sum(1 for _ in tf.data.TFRecordDataset(file, compression_type=\"GZIP\"))\n        total_count += count\n    return total_count\n\ndef split_dataset(dataset, indices):\n    tf.print('Splitting the dataset')\n    indices = tf.convert_to_tensor(indices, dtype=tf.int64)\n    indices_dataset = tf.data.Dataset.from_tensor_slices(indices)\n\n    # Use enumerate to pair each element with its index, then filter\n    filtered_dataset = dataset.enumerate().filter(\n        lambda idx, _: tf.reduce_any(tf.equal(idx, indices))\n    ).map(lambda idx, data: data)  # Discard the index after filtering\n\n    #if use_tabular_data:\n        #return filtered_dataset.map(lambda img, tab, lbl: ((img, tab), lbl))\n    #else:\n        #return filtered_dataset.map(lambda img, lbl: (img, lbl))\n    return filtered_dataset\n\ndef count_samples_in_training_dataset(dataset, batch_size=None, strategy=None):\n    \"\"\"\n    Count the number of samples in a dataset, taking into account batched datasets.\n\n    Args:\n    dataset: A `tf.data.Dataset` object, possibly batched.\n    batch_size: If the dataset is batched, specify the batch size to correctly count samples.\n\n    Returns:\n    Integer count of the number of samples in the dataset.\n    \"\"\"\n    print(\"Calculating the dataset size...\")\n\n    # If the dataset is batched, multiply by batch size, or use the actual number of samples in each batch\n    if batch_size:\n        # Count the number of batches\n        batch_count = dataset.reduce(0, lambda x, _: x + 1).numpy()\n        # Multiply by batch size to get the total number of samples\n        total_samples = batch_count * batch_size * strategy.num_replicas_in_sync\n        print(f\"Total samples: {total_samples} and number of batches: {batch_count}\")\n        return total_samples\n    else:\n        # Unbatched case: Simply count the samples\n        total_samples = dataset.reduce(0, lambda x, _: x + 1).numpy()\n        print(f\"Total samples: {total_samples}\")\n        return total_samples\n\ndef format_input_dataset(dataset, use_tabular_data=use_tabular_data):\n    if use_tabular_data:\n          return dataset.map(lambda img, tab, lbl: ((img, tab), lbl))\n    else:\n          return dataset.map(lambda img, lbl: (img, lbl))\n\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, [img_height, img_width])\n    return image, label\n\ndef cast_labels(dataset, use_tabular_data=use_tabular_data):\n    if use_tabular_data:\n          return dataset.map(lambda img, tab, lbl: (img, tab, tf.cast(lbl, dtype=tf.float32)))\n    else:\n          return dataset.map(lambda img, lbl: (img, tf.cast(lbl, dtype=tf.float32)))\n\n# Helper to optimize dataset caching and prefetching\ndef optimize_dataset(dataset, steps, batch_size=batch_size, file_pattern=None, shuffle=True, buffer_size=buffer_size,\n                     drop_remainder=True,  prefix='train', force_cache=False, cache_in_memory=False, run_id=run_id,\n                     cache_dir=cache_dir, check_modification_time=False):\n    \"\"\"Optimize dataset with caching, batching, shuffling, and prefetching.\"\"\"\n    #dataset = persist_dataset(dataset, file_pattern=file_pattern, dataset_steps=steps, cache_in_memory=cache_in_memory,\n                              #force_cache=force_cache, prefix=prefix, run_id=run_id,\n                             #cache_dir=cache_dir, check_modification_time=check_modification_time)\n    if prefix == 'train':\n        print(f\"Repeating, shuffling and batching {prefix} dataset\")\n    else:\n        print(f\"Repeating {prefix} dataset\")\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(buffer_size=buffer_size) if shuffle else dataset\n    if prefix == 'train':\n        dataset = dataset.batch(batch_size * strategy.num_replicas_in_sync, drop_remainder=drop_remainder)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef clean_session():\n    # Reset the session and clear the graph before each fold\n    tf.keras.backend.clear_session()\n    tf.compat.v1.reset_default_graph()\n    gc.collect()\n\ndef deletes_old_datasets():\n    try:\n        del train_dataset\n        del val_dataset\n    except:\n        pass\n    clean_session()","metadata":{"id":"Pw7rGEMre2rr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oversampling utils","metadata":{"id":"8QjZ-Gjhe2rr"}},{"cell_type":"code","source":"def oversample_minority_class_random(dataset, batch_size=batch_size, strategy=strategy, use_tabular_data=use_tabular_data):\n    \"\"\"\n    Randomly oversample the minority class by duplicating its samples within the dataset.\n    \"\"\"\n    tf.print(\"\\nRandom oversampling the minority class...\")\n    # Separate the dataset into majority and minority classes\n    minority_class = 1\n    majority_class = 0\n\n    if use_tabular_data:\n        # For datasets with both image and tabular data\n        minority_dataset = dataset.filter(lambda image, tab, label: tf.equal(label, minority_class))\n        majority_dataset = dataset.filter(lambda image, tab, label: tf.equal(label, majority_class))\n    else:\n        # For datasets with only image data\n        minority_dataset = dataset.filter(lambda image, label: tf.equal(label, minority_class))\n        majority_dataset = dataset.filter(lambda image, label: tf.equal(label, majority_class))\n\n    # Count the number of samples in each class\n    minority_count = count_samples_in_training_dataset(minority_dataset, batch_size=None)\n    majority_count = count_samples_in_training_dataset(majority_dataset, batch_size=None)\n\n    sample_per_class_input_ds = dict(majority_class=majority_count, minority_class=minority_count)\n\n    # Calculate how many samples need to be added to the minority class\n    additional_samples_needed = majority_count - minority_count\n\n    # Randomly sample from the minority dataset to add more samples\n    minority_dataset_repeated = minority_dataset.repeat()\n    minority_dataset_sampled = minority_dataset_repeated.shuffle(buffer_size=buffer_size).take(additional_samples_needed)\n\n    # Combine the sampled dataset with the original minority dataset\n    oversampled_minority_dataset = minority_dataset.concatenate(minority_dataset_sampled)\n\n    # Combine the oversampled minority class with the majority class\n    oversampled_dataset = majority_dataset.concatenate(oversampled_minority_dataset)\n\n    # Shuffle the combined dataset\n    oversampled_dataset = oversampled_dataset.shuffle(buffer_size=buffer_size)\n\n    # Recalculate steps per epoch after oversampling\n    oversampled_sample_count = majority_count + additional_samples_needed\n    training_steps = calculate_steps_per_epoch(oversampled_sample_count, batch_size)\n\n    sample_per_class_oversampled_ds = dict(majority_class=majority_count, minority_class=majority_count)\n\n    return oversampled_dataset, training_steps, oversampled_sample_count, sample_per_class_input_ds, sample_per_class_oversampled_ds\n\n\ndef oversample_minority_with_tabular_smote_images_random(dataset, batch_size=batch_size, strategy=strategy, use_tabular_data=use_tabular_data):\n    \"\"\"\n    Randomly oversample the minority class by duplicating its samples within the dataset for images.\n    Apply SMOTE for tabular data to oversample synthetic features. The function handles both cases: with and without tabular data.\n    \"\"\"\n    # Separate the dataset into majority and minority classes\n    minority_class = 1\n    majority_class = 0\n\n    if use_tabular_data:\n        # If using tabular data, the dataset contains (image, tabular_data, label)\n        tabular_data = []\n        images = []\n        labels = []\n\n        for image, tabular, label in dataset:\n            images.append(image.numpy())\n            tabular_data.append(tabular.numpy())\n            labels.append(label.numpy())\n\n        # Convert to numpy arrays\n        tabular_data = np.array(tabular_data)\n        labels = np.array(labels)\n        images = np.array(images)\n\n        # Separate majority and minority classes\n        minority_images = images[labels == minority_class]\n        minority_tabular_data = tabular_data[labels == minority_class]\n        majority_images = images[labels == majority_class]\n        majority_tabular_data = tabular_data[labels == majority_class]\n\n        # Apply SMOTE on the tabular data\n        smote = SMOTE(sampling_strategy='auto')\n        tabular_data_resampled, labels_resampled = smote.fit_resample(tabular_data, labels)\n\n        # Find how many new samples were generated by SMOTE for the minority class\n        new_minority_count = np.sum(labels_resampled == minority_class) - len(minority_tabular_data)\n\n        # For each new synthetic tabular data point, randomly pick a corresponding minority image\n        random_indices = np.random.randint(0, len(minority_images), new_minority_count)\n        minority_images_resampled = minority_images[random_indices]\n        #minority_images_resampled = np.tile(minority_images, (new_minority_count // len(minority_images) + 1, 1, 1, 1))[:new_minority_count]\n\n\n        # Combine the original majority data with the resampled minority data\n        combined_images = np.concatenate([majority_images, minority_images, minority_images_resampled])\n        combined_tabular_data = np.concatenate([majority_tabular_data, minority_tabular_data, tabular_data_resampled[len(minority_tabular_data):]])\n        combined_labels = np.concatenate([np.full(len(majority_images), majority_class), np.full(len(minority_images), minority_class), np.full(len(minority_images_resampled), minority_class)])\n\n        assert len(combined_images) == len(combined_tabular_data) == len(combined_labels), \"Mismatch in dataset length!\"\n\n        # Convert the combined data back to TensorFlow tensors\n        combined_images = tf.convert_to_tensor(combined_images, dtype=tf.float32)\n        combined_tabular_data = tf.convert_to_tensor(combined_tabular_data, dtype=tf.float32)\n        combined_labels = tf.convert_to_tensor(combined_labels, dtype=tf.int64)\n\n        # Combine the resampled data into a dataset\n        oversampled_dataset = tf.data.Dataset.from_tensor_slices((combined_images, combined_tabular_data, combined_labels))\n\n    else:\n        # If not using tabular data, the dataset contains (image, label)\n        minority_dataset = dataset.filter(lambda image, label: tf.equal(label, minority_class))\n        majority_dataset = dataset.filter(lambda image, label: tf.equal(label, majority_class))\n\n        # Apply random oversampling for images only\n        minority_dataset_repeated = minority_dataset.repeat()\n        minority_dataset_sampled = minority_dataset_repeated.shuffle(buffer_size=buffer_size).take(majority_dataset.reduce(0, lambda x, _: x + 1).numpy())\n\n        oversampled_minority_dataset = minority_dataset.concatenate(minority_dataset_sampled)\n\n        # Combine the oversampled minority class with the majority class\n        oversampled_dataset = majority_dataset.concatenate(oversampled_minority_dataset)\n\n    # Shuffle the combined dataset (optional)\n    oversampled_dataset = oversampled_dataset.shuffle(buffer_size=buffer_size)\n\n    # Recalculate steps per epoch after oversampling\n    oversampled_sample_count = len(combined_labels) if use_tabular_data else count_samples_in_training_dataset(oversampled_dataset)\n    training_steps = calculate_steps_per_epoch(oversampled_sample_count, batch_size)\n\n    return oversampled_dataset, training_steps, oversampled_sample_count","metadata":{"id":"R9ZvHMjbe2rr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading utils","metadata":{"id":"qHiFkU9ee2rr"}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport math\n\ndef parse_tfrecord_fn(example_proto):\n    \"\"\"Parse a TFRecord into image and optional tabular data.\"\"\"\n    feature_description = {'image': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n    if use_tabular_data:\n        feature_description['tabular_data'] = tf.io.FixedLenFeature([14], tf.float32)\n\n    example = tf.io.parse_single_example(example_proto, feature_description)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.image.resize(image, [img_height, img_width])\n    image = image / 255.0\n\n    #label = tf.cast(example['label'], tf.float32)\n    if use_tabular_data:\n        print('Parsing data including tabular metadata')\n        print(example['label'])\n        return image, example['tabular_data'], example['label']\n    else:\n        return image, example['label']\n\n# Wrap parsing function with error handling\ndef safe_parse_fn(example):\n    try:\n        return parse_tfrecord_fn(example)\n    except Exception as e:\n        tf.print(f\"Error parsing example: {e}\")\n        return None\n\ndef load_tfrecord_dataset(file_pattern,\n                          use_tabular_data=False,\n                          cache_in_memory=False,\n                          is_training=False,\n                          batch_size=batch_size,\n                          num_parallel_reads=tf.data.AUTOTUNE,\n                          data_type='Train'):\n    \"\"\"Load TFRecord dataset with optional data augmentation and tabular data handling.\"\"\"\n    tf.print(f\"\\nLoading {data_type} TFRecords...\")\n\n    # Create a dataset of file paths\n    files = tf.data.Dataset.list_files(file_pattern, shuffle=is_training)\n    tf.print(f\"Number of files: {files.cardinality().numpy()}\")\n\n    # Interleave the files to read them in parallel\n    dataset = files.interleave(\n        lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\"),\n        cycle_length=num_parallel_reads,\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    # Parse the TFRecord files\n    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset = dataset.cache()\n\n    #dataset = persist_dataset(dataset, file_pattern, cache_in_memory, '/tmp/tf_cache_2', prefix=data_type, run_id=run_id, check_modification_time=False)\n\n    if is_training:\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    else:\n        dataset = dataset.batch(batch_size * strategy.num_replicas_in_sync, drop_remainder=True)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        print(\"Batched the dataset using batch size:\", batch_size * strategy.num_replicas_in_sync)\n\n    rows_count = count_samples_in_tfrecord(file_pattern)\n    print(f\"Loaded dataset at {file_pattern}, number of rows: {rows_count}\")\n    if rows_count == 0:\n        raise Exception(\"The provided dataset is empty\")\n\n    steps_per_epoch = calculate_steps_per_epoch(rows_count, batch_size)\n\n    print(f\"Dataset number of batches: {steps_per_epoch}\")\n\n    return dataset, steps_per_epoch","metadata":{"id":"IsmWqZI5iAlY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"id":"18nhJxYZe2rr"}},{"cell_type":"markdown","source":"## Models utils","metadata":{"id":"Pk-hIsFvgVJf"}},{"cell_type":"markdown","source":"### Base model","metadata":{"id":"HabdJcwSyFts"}},{"cell_type":"code","source":"# Helper function to apply dense block\ndef apply_dense_block(x, units, activation='swish', l2_lambda=None, dropout_rate=None, kernel_initializer='glorot_uniform'):\n    #print(f\"Applying dense bloc with: kernel_initializer {kernel_initializer}, dropout_rate {dropout_rate}, activation {activation}, l2_lambda {l2_lambda}\")\n    print(\"Applying dense bloc with kernel_initializer: {}, dropout_rate: {}, activation: {}, l2_lambda: {}\".\n          format(kernel_initializer, dropout_rate, activation, l2_lambda))\n    kernel_reg = regularizers.l2(l2_lambda) if l2_lambda else None\n    x = layers.Dense(units, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer, activation=activation)(x)\n    #x = layers.BatchNormalization()(x)\n    if dropout_rate:\n        x = layers.Dropout(dropout_rate)(x)\n    return x\n\n# Base model class for all models\nclass BaseModel:\n    def __init__(self, model_name, dropout_rate=None, l2_lambda=None, freeze_base_model=False, img_shape=img_shape,\n                 num_tabular_features=14, use_tabular_data=False, kernel_initializer='glorot_uniform', run_id=run_id, **kwargs):\n        \"\"\"\n            Base model constructor. Supports flexible parameter passing through **kwargs.\n\n            Parameters:\n            - model_name: Name of the model to build (e.g., 'resnet50').\n            - dropout_rate: Dropout rate for dense layers.\n            - l2_lambda: L2 regularization.\n            - freeze_base_model: Whether to freeze the base model layers.\n            - img_shape: Shape of the input images.\n            - num_tabular_features: Number of tabular features (for hybrid models).\n            - use_tabular_data: Whether to include tabular data as input.\n            - kernel_initializer: Initializer for the kernel weights matrix.\n            - run_id: Unique identifier for the model run (useful for saving/loading).\n            - **kwargs: Additional parameters to pass through the model pipeline.\n        \"\"\"\n        def get_param(value, default, kwargs, key):\n            if value is not None:\n                return value\n            return kwargs.get(key, default)\n        self.model_name = get_param(model_name, None, kwargs, 'model_name')\n        self.dropout_rate = get_param(dropout_rate, None, kwargs, 'dropout_rate')\n        self.l2_lambda = get_param(l2_lambda, None, kwargs, 'l2_lambda')\n        self.freeze_base_model = get_param(freeze_base_model, False, kwargs, 'freeze_base_model')\n        self.img_shape = get_param(img_shape, (224, 224, 3), kwargs, 'img_shape')\n        self.num_tabular_features = get_param(num_tabular_features, None, kwargs, 'num_tabular_features')\n        self.use_tabular_data = get_param(use_tabular_data, False, kwargs, 'use_tabular_data')\n        self.kernel_initializer = get_param(kernel_initializer, 'glorot_uniform', kwargs, 'kernel_initializer')\n        self.activation = get_param(activation, 'swish', kwargs, 'activation')\n        self.run_id = get_param(run_id, None, kwargs, 'run_id')\n        self.kwargs = kwargs\n\n    def _set_model_inputs_(self):\n        \"\"\"Set input layers for the model, with optional tabular data.\"\"\"\n        image_input = tf.keras.Input(shape=self.img_shape, name='image_input')\n        if self.use_tabular_data:\n            tabular_input = tf.keras.Input(shape=(self.num_tabular_features,), name='tabular_input')\n            return [image_input, tabular_input]\n        return image_input\n\n    def _set_model_inputs(self, image_input):\n        \"\"\"Set input layers for the model, with optional tabular data.\"\"\"\n        if self.use_tabular_data:\n            tabular_input = tf.keras.Input(shape=(self.num_tabular_features,), name='tabular_input')\n            return [image_input, tabular_input]\n        return image_input\n\n    def _get_base_model(self, model_name=None):\n        \"\"\"Dynamically load the base model using the model name.\"\"\"\n        if model_name is None:\n            model_name = self.model_name\n        try:\n            parts = model_name.split('_')\n            # Trick to get the class name from the provided string \n            # Example res_net_50 -> ResNet50, vgg_19 -> VGG19\n            if len(parts) > 2:\n                model_class_name = ''.join([part.capitalize() for part in parts])\n            else:\n                model_class_name = ''.join([part.upper() for part in parts])\n                \n            base_model_cls = getattr(tf.keras.applications, model_class_name, None)\n            \n            if base_model_cls is None:\n                raise ValueError(f\"Unknown model name: {model_name}\")\n            return base_model_cls(weights=None, include_top=False, input_shape=self.img_shape)\n        except Exception as e:\n            raise ValueError(f\"Error loading model {model_name}: {str(e)}\")\n\n    def _process_base_model(self, base_model):\n        \"\"\"Freeze/unfreeze base model and apply global pooling.\"\"\"\n        base_model.trainable = not self.freeze_base_model\n        return layers.GlobalAveragePooling2D()(base_model.output)\n\n    def _combine_image_and_tabular_features(self, image_features, inputs):\n        \"\"\"Combine image features with tabular features, if applicable.\"\"\"\n        if self.use_tabular_data:\n            tabular_input = inputs[1]\n            tabular_features = apply_dense_block(tabular_input, 1024, l2_lambda=self.l2_lambda, dropout_rate=self.dropout_rate,\n                                                 kernel_initializer=self.kernel_initializer, activation=self.activation)\n            tabular_features = apply_dense_block(tabular_features, 812, l2_lambda=self.l2_lambda, dropout_rate=self.dropout_rate,\n                                                 kernel_initializer=self.kernel_initializer, activation=self.activation)\n            return layers.Concatenate()([image_features, tabular_features])\n        return image_features\n\n    def set_model_architecture(self, model_name=None):\n        \"\"\"Set the full model architecture.\"\"\"\n        print(f'Setting model {model_name} architecture')\n        base_model = self._get_base_model(model_name)\n        image_input = base_model.input\n        inputs = self._set_model_inputs(image_input)\n        #inputs = self._set_model_inputs()\n        image_features = self._process_base_model(base_model)\n        combined_features = self._combine_image_and_tabular_features(image_features, inputs)\n\n        # Add Dense and BatchNorm layers\n        combined_features = apply_dense_block(combined_features, 512, l2_lambda=self.l2_lambda, dropout_rate=self.dropout_rate,\n                                              kernel_initializer=self.kernel_initializer, activation=self.activation)\n        #combined_features = apply_dense_block(combined_features, 256, l2_lambda=self.l2_lambda, dropout_rate=self.dropout_rate,\n                                              #kernel_initializer=self.kernel_initializer, activation=self.activation)\n\n        # Dropout Models (Multiple Dropouts for Ensembling)\n        combineds = [layers.Dropout(0.2)(combined_features) for _ in range(2)]\n\n        # Apply a Dense layer after each dropout\n        outputs = [layers.Dense(1, activation='sigmoid')(combined) for combined in combineds]\n\n        # Average the outputs for ensembling\n        final_output = layers.Average()(outputs)\n\n        model = tf.keras.Model(inputs=inputs, outputs=final_output)\n\n        model.base_model = base_model  # Keep a reference to the base model for flexibility\n        return model\n\n    def create_model(self, model_name=None):\n        \"\"\"\n        Create a base model using the architecture specified by model_name.\n        If model_name is None, use the instance's model_name.\n        \"\"\"\n        if model_name is None:\n            model_name = self.model_name\n        print(f\"Creating model {model_name}\")\n        model = self.set_model_architecture(model_name)\n        model.model_name = model_name\n        return model","metadata":{"id":"fo7RsAzFyFts","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble model","metadata":{"id":"DP4qYDGXyFts"}},{"cell_type":"code","source":"# EnsembleModel class inherits from BaseModel\nclass EnsembleModel(BaseModel):\n    def __init__(self, model_name, model_names, mode='weighted', run_id=run_id, num_ensemble=2, weights=None, **kwargs):\n        super().__init__(model_name=model_name, **kwargs)\n        self.model_name = model_name\n        self.model_names = model_names\n        self.mode = mode\n        self.num_ensemble = num_ensemble\n        self.weights = weights if weights else [1.0] * len(model_names)\n        self.run_id = run_id\n\n    def _load_pretrained_model(self, model_name):\n        \"\"\"Load a pretrained model and return it.\"\"\"\n        model = self.set_model_architecture(model_name)\n        \n        def resolve_prm_name(model_name):\n            return ''.join(model_name.split('_'))\n        \n        try:\n            model.load_weights(os.path.join(pretrained_models_dir, f'{resolve_prm_name(model_name)}_weights.h5'))\n        except Exception as e:\n            raise ValueError(f\"Error loading weights for model {model_name}: {str(e)}\")\n            \n        return model\n\n    def create_ensemble(self, pretrained):\n        \"\"\"\n        Create an ensemble model with the selected strategy (average, max, or weighted).\n        Can load pretrained models or create new ones based on the `pretrained` flag.\n        \"\"\"\n        # Collect outputs from each model in the ensemble\n        inputs = self._set_model_inputs_()  # Get the input layers\n        model_outputs = []\n\n        for model_name in self.model_names:\n\n            if pretrained:\n                ## Creates a pretrained model by loading the loads of the pretrained ensemble models\n                model = self._load_pretrained_model(model_name)\n            else:\n                ## Only set the architecture of the ensemble model from the individuals\n                model = self.create_model(model_name=model_name)  # Create a new model from the architecture\n\n            model_outputs.append(model(inputs))\n\n        # Apply the selected ensembling strategy to combine the model outputs\n        ensemble_output = self._apply_ensemble_strategy(model_outputs)\n\n        # Create and return the ensemble model\n        ensemble_model = tf.keras.Model(inputs=inputs, outputs=ensemble_output)\n\n        ensemble_model.model_name = self.model_name # For later reuse in the training pipeline\n\n        return ensemble_model\n\n    def _apply_ensemble_strategy(self, outputs):\n        \"\"\"Apply different ensemble strategies.\"\"\"\n        if self.mode == 'average':\n            return layers.Average()(outputs)\n        elif self.mode == 'max':\n            return layers.Maximum()(outputs)\n        elif self.mode == 'weighted' and self.weights:\n            self.weights = [w / sum(self.weights) for w in self.weights]\n            return layers.Add()([output * weight for output, weight in zip(outputs, self.weights)])\n        elif self.mode == 'weighted_layer' and self.weights:\n            self.weights = [w / sum(self.weights) for w in self.weights]\n            weighted_outputs = [layers.Lambda(lambda x, w=weight: x * w)(output) for output, weight in zip(outputs, self.weights)]\n            return layers.Add()(weighted_outputs)\n        else:\n            raise ValueError(f\"Unknown ensemble mode: {self.mode}\")","metadata":{"id":"OFDbhcwAyFts","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Single model","metadata":{"id":"-u4Et-fIyFtt"}},{"cell_type":"code","source":"class SingleModel(BaseModel):\n    def __init__(self, model_name, **kwargs):\n        super().__init__(model_name=model_name, **kwargs)","metadata":{"id":"eAx1EBEEyFtt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Factory utils","metadata":{"id":"JaWjxioyyFtt"}},{"cell_type":"code","source":"class ModelFactory:\n    @staticmethod\n    def create_model(model_type, model_names, pretrained=False, **kwargs):\n        \"\"\"Factory method to create models based on type.\"\"\"\n\n        strategy = kwargs.get('strategy')\n\n        with strategy.scope():\n            if model_type == 'ensemble':\n                weights = kwargs.get('weights')\n                ensemble_model_name = '_'.join(['_'.join((model_name, str(weight))) for (model_name, weight) in zip(model_names, weights)])\n                ensemble_model_name =  f\"pretrained_ensemble_{ensemble_model_name}\" if pretrained else f\"compact_ensemble_{ensemble_model_name}\"\n                return EnsembleModel(model_name=ensemble_model_name, model_names=model_names, **kwargs).create_ensemble(pretrained=pretrained)\n\n            elif model_type == 'single':\n                return SingleModel(model_name=model_names[0], **kwargs).create_model()\n            else:\n                raise ValueError(f\"Unknown model type: {model_type}\")","metadata":{"id":"Kx-lXO7JyFtt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss functions","metadata":{"id":"CEKTa3dUe2rr"}},{"cell_type":"code","source":"@register_keras_serializable()\n@tf.function\ndef focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n    \"\"\"\n    Focal Loss for binary classification.\n\n    Args:\n    - y_true: Ground truth labels, shape = (batch_size, 1)\n    - y_pred: Predicted labels, shape = (batch_size, 1)\n    - gamma: Focusing parameter (default is 2.0)\n    - alpha: Balancing factor (default is 0.25)\n\n    Returns:\n    - Loss value\n    \"\"\"\n    # Clip predictions to prevent log(0) and ensure stability\n    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n\n    # Calculate cross-entropy loss\n    bce_loss = - (y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n\n    # Calculate p_t and modulating factor\n    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n    modulating_factor = (1 - p_t) ** gamma\n\n    # Apply alpha balancing factor\n    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n\n    # Compute focal loss\n    loss = alpha_factor * modulating_factor * bce_loss\n\n    return tf.reduce_mean(loss)\n\n@register_keras_serializable()\n@tf.function\ndef class_balanced_loss(y_true, y_pred, beta=0.99, samples_per_cls=None):\n    \"\"\"\n    Class-Balanced Loss function for binary classification.\n\n    Parameters:\n    - y_true: Ground truth labels, shape = (batch_size,)\n    - y_pred: Predicted labels, shape = (batch_size,)\n    - samples_per_cls: List or array containing the number of samples for each class (class 0 and class 1).\n    - beta: Hyperparameter to adjust class balancing (default is 0.99)\n\n    Returns:\n    - Loss value\n    \"\"\"\n    # Calculate the effective number of samples\n    effective_num = 1.0 - tf.pow(beta, samples_per_cls)\n    weights = (1.0 - beta) / tf.maximum(effective_num, tf.keras.backend.epsilon())\n    weights = weights / tf.reduce_sum(weights)  # Normalize weights\n\n    # Extract the class weights for class 0 and class 1\n    weight_for_0 = weights[0]\n    weight_for_1 = weights[1]\n\n    # Apply weights to the true labels (binary classification)\n    weights_per_sample = y_true * weight_for_1 + (1.0 - y_true) * weight_for_0\n\n    # Compute the binary crossentropy loss\n    loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n\n    # Apply class-balanced weights to the loss\n    loss = weights_per_sample * loss\n    return tf.reduce_mean(loss)\n\n@register_keras_serializable()\ndef combined_loss(samples_per_cls, beta=0.99, gamma=2.0, alpha=0.8, focal_loss_percent=0.5):\n    \"\"\"\n    Combined Class-Balanced Loss and Focal Loss.\n\n    Parameters:\n    - y_true: Ground truth labels, shape = (batch_size, num_classes)\n    - y_pred: Predicted labels, shape = (batch_size, num_classes)\n\n    Returns:\n    - Combined loss value\n    \"\"\"\n    @tf.function\n    def loss(y_true, y_pred):\n      y_true = tf.cast(y_true, tf.float32)\n      y_true = tf.reshape(y_true, [-1, 1])\n      y_pred = tf.reshape(y_pred, [-1, 1])\n\n      cb_loss = class_balanced_loss(y_true, y_pred, beta=beta, samples_per_cls=samples_per_cls)\n      fl_loss = focal_loss(y_true, y_pred, gamma=gamma, alpha=alpha)\n      return ((1 - focal_loss_percent) * cb_loss) + (focal_loss_percent * fl_loss)\n    return loss\n\n##Binary class balance loss\n@register_keras_serializable()\ndef get_class_balanced_weights(beta, samples_per_cls):\n    effective_num = 1.0 - np.power(beta, samples_per_cls)\n    effective_num = np.where(effective_num == 0, 1e-8, effective_num)  # Avoid division by zero\n    weights = (1.0 - beta) / effective_num\n    weights = weights / np.sum(weights) * len(samples_per_cls)\n    return weights\n\n@tf.function\n@register_keras_serializable()\ndef class_balanced_binary_cross_entropy(beta, samples_per_cls):\n    weights = get_class_balanced_weights(beta, samples_per_cls)\n    weight_for_0 = weights[0]\n    weight_for_1 = weights[1]\n\n    def loss(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        weights_per_sample = y_true * weight_for_1 + (1 - y_true) * weight_for_0\n        weighted_bce = weights_per_sample * bce\n        return tf.reduce_mean(weighted_bce)\n\n    return loss\n\n@register_keras_serializable()\n@tf.function\ndef focal_loss_keras_bce(y_true, y_pred, gamma, alpha):\n    \"\"\"\n    Focal Loss function.\n    Parameters:\n    - y_true: Ground truth labels, shape = (batch_size, num_classes)\n    - y_pred: Predicted labels, shape = (batch_size, num_classes)\n    - gamma: Focusing parameter (default is 2.0)\n    - alpha: Balancing factor (default is 0.25)\n\n    Returns:\n    - Loss value\n    \"\"\"\n    #y_true = tf.cast(y_true, tf.float32)\n    # Compute cross entropy\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n\n    # Compute focal loss\n    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n    modulating_factor = tf.pow(1.0 - p_t, gamma)\n\n    loss = alpha_factor * modulating_factor * bce\n    return tf.reduce_mean(loss)\n\n@tf.function\ndef weighted_binary_crossentropy(y_true, y_pred):\n    class_weight_0 = 0.1  # Weight for class 0 (negative)\n    class_weight_1 = 0.9  # Weight for class 1 (positive)\n\n    weights = tf.where(tf.equal(y_true, 1), class_weight_1, class_weight_0)\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    weighted_bce = bce * weights\n    return tf.reduce_mean(weighted_bce)\n\n\n@register_keras_serializable()\ndef class_balanced_loss_vect(y_true, y_pred, samples_per_cls, beta):\n    \"\"\"\n    Class-Balanced Loss function with samples_per_cls parameter.\n\n    Parameters:\n    - y_true: Ground truth labels, shape = (batch_size, num_classes)\n    - y_pred: Predicted labels, shape = (batch_size, num_classes)\n    - samples_per_cls: List or array containing the number of samples for each class.\n    - beta: Hyperparameter to adjust class balancing (usually close to 1.0, default is 0.9999)\n\n    Returns:\n    - Loss value\n    \"\"\"\n    #y_true = tf.cast(y_true, tf.float32)\n    # Calculate the effective number of samples\n    effective_num = 1.0 - tf.pow(beta, samples_per_cls)\n    weights = (1.0 - beta) / tf.maximum(effective_num, tf.keras.backend.epsilon())\n    weights = weights / tf.reduce_sum(weights)  # Normalize weights\n\n    # Apply weights to the true labels\n    weights = tf.reduce_sum(weights * y_true, axis=-1)\n\n    # Compute the binary crossentropy loss\n    loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n\n    # Apply class-balanced weights to the loss\n    loss = weights * loss\n    return tf.reduce_mean(loss)\n\n@tf.function\ndef weighted_binary_crossentropy(y_true, y_pred):\n    # Define class weights\n    class_weight_0 = 0.1  # Weight for class 0 (negative)\n    class_weight_1 = 0.9  # Weight for class 1 (positive)\n\n    # Create a tensor of weights based on the true labels\n    weights = tf.where(tf.equal(y_true, 1), class_weight_1, class_weight_0)\n\n    # Calculate binary crossentropy\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n\n    # Apply the weights to the loss\n    weighted_bce = bce * weights\n\n    # Return the mean loss\n    return tf.reduce_mean(weighted_bce)\n\ndef focal_loss_fix(gamma=2., alpha=0.25):\n    def focal_loss_fixed(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        y_true = K.cast(y_true, K.floatx())\n\n        alpha_t = y_true * alpha + (K.ones_like(y_true) - y_true) * (1 - alpha)\n        p_t = y_true * y_pred + (K.ones_like(y_true) - y_true) * (1 - y_pred)\n        fl = - alpha_t * K.pow((K.ones_like(y_true) - p_t), gamma) * K.log(p_t)\n        return K.mean(fl)\n    return focal_loss_fixed\n\ndef set_binary_crossentropy_weighted_loss(positive_weights=np.array([5.291666666666667]), negative_weights=np.array([0.5521739130434783]), epsilon=1e-7):\n    \"\"\"\n    Note: Imported from the AI for Medicine Specialization course on Coursera: Assignment 1 Week 1.\n    Returns weighted binary cross entropy loss function given negative weights and positive weights.\n\n    Args:\n      positive_weights (np.array): array of positive weights for each class, size (num_classes)\n      negative_weights (np.array): array of negative weights for each class, size (num_classes)\n\n    Returns:\n      weighted_loss (function): weighted loss function\n    \"\"\"\n    def binary_crossentropy_weighted_loss(y_true, y_pred):\n        \"\"\"\n        Returns weighted binary cross entropy loss value.\n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n\n        Returns:\n            loss (Tensor): overall scalar loss summed across all classes\n        \"\"\"\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n\n        # initialize loss to zero\n        loss = 0.0\n        #y_true = tf.cast(y_true, tf.float32)\n        for i in range(len(positive_weights)):\n            # for each class, add average weighted loss for that class\n            loss += -1 * K.mean((positive_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) +\n                                negative_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon)))\n        return loss\n\n    return binary_crossentropy_weighted_loss","metadata":{"id":"D1qWsMHo5BZW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{"id":"XTzGXIf9e2rs"}},{"cell_type":"code","source":"def score(y_true, y_pred, min_tpr: float = 0.80) -> float:\n    v_gt = abs(np.asarray(y_true) - 1)\n    v_pred = np.array([1.0 - x for x in y_pred])\n    max_fpr = abs(1 - min_tpr)\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    return partial_auc\n\n@register_keras_serializable()\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        precision = self.precision.result()\n        recall = self.recall.result()\n        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n\n    def reset_state(self):\n        self.precision.reset_state()\n        self.recall.reset_state()\n\ndef get_labels_and_predictions(model, dataset, steps):\n    \"\"\"Return true labels and predictions from the model.\"\"\"\n    y_true, y_pred = [], []\n    for x_batch, y_batch in dataset.take(steps):\n        y_pred_batch = model.predict_on_batch(x_batch).ravel()\n        y_true.append(y_batch)\n        y_pred.append(y_pred_batch)\n    return tf.concat(y_true, axis=0), tf.concat(y_pred, axis=0)\n\ndef evaluate_model(model, eval_dataset, evaluation_steps, dataset_name='eval_dataset'):\n    print('Evaluating', dataset_name, 'with', evaluation_steps, 'steps')\n    y_val, y_pred = get_labels_and_predictions(model, eval_dataset, evaluation_steps)\n    partial_auc = score(y_val, y_pred)\n    print(f\"Partial AUC for {dataset_name}: {partial_auc}\")\n\ndef get_memory_usage():\n    # memory_usage returns a list, so we get the first item\n    mem = memory_usage(-1, interval=0.1, timeout=1)[0] # Return the first value from the list in MB\n    mem = f\"{mem:.2f} MB\"\n    return  mem\n\ndef reset_session_and_get_memory():\n    \"\"\"Cleans up the session and retrieves the initial memory usage.\"\"\"\n    clean_session()\n    return get_memory_usage()\n\ndef log_memory_usage(stage, memory_usage):\n    \"\"\"Logs the memory usage with a specific stage identifier.\"\"\"\n    print(f\"{stage} memory usage: {memory_usage}\")","metadata":{"id":"BqlX0cuLgTrC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{"id":"fjddtd7ve2rt"}},{"cell_type":"code","source":"class PartialAUCCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, validation_steps, monitor, train_data = None, steps_per_epoch=None, min_tpr=0.8):\n        super(PartialAUCCallback, self).__init__()\n        self.validation_data = validation_data\n        self.validation_steps = validation_steps\n        self.monitor = monitor\n        self.train_data = train_data\n        self.steps_per_epoch = steps_per_epoch\n        self.min_tpr = min_tpr\n        self.max_fpr = 1.0 - min_tpr\n        self.best_partial_auc = -float('inf')  # Initialize to a very low value\n        self.best_epoch = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        # Compute Partial AUC on Training Data if available\n        if self.train_data:\n            train_labels, train_predictions = get_labels_and_predictions(self.model,\n                                                                         self.train_data,\n                                                                         self.steps_per_epoch)\n            train_partial_auc = self.compute_auc(train_labels, train_predictions)\n            self.log_partial_auc(logs, train_partial_auc, 'partial_auc', epoch)\n\n            PartialAUCCallback.log_classification_report(train_labels, train_predictions, epoch, 'Train')\n\n        #Validation computations\n        val_labels, val_predictions = get_labels_and_predictions(self.model,\n                                                                 self.validation_data,\n                                                                 self.validation_steps)\n        # Calculate the partial AUC\n        val_partial_auc = self.compute_auc(val_labels, val_predictions)\n        self.log_partial_auc(logs, val_partial_auc, 'val_partial_auc', epoch)\n        PartialAUCCallback.log_classification_report(val_labels, val_predictions, epoch)\n\n\n    def log_partial_auc(self, logs, partial_auc, pauc_metric_name, epoch):\n        logs[pauc_metric_name] = partial_auc\n        if self.monitor == pauc_metric_name:\n            # Store the best partial AUC\n            if partial_auc > self.best_partial_auc:\n                self.best_partial_auc = partial_auc\n                self.best_epoch = epoch\n\n         # Add the partial_auc to logs so it can be used in Keras Tuner\n        logs[pauc_metric_name] = partial_auc\n        tf.print(f'\\nEpoch {epoch + 1}: Partial AUC = {partial_auc:.4f}')\n\n    @staticmethod\n    def log_classification_report(labels, predictions, epoch, name='Validation'):\n        # Convert probabilities to binary predictions\n        bin_preds = np.round(predictions).astype(int)\n        # Generate the classification report\n        report = classification_report(labels, bin_preds, labels= [1, 0],\n                                       target_names=['positives', 'negatives'])\n        # Print the report\n        tf.print(f\"\\n{name} classification report for epoch {epoch + 1}:\\n{report}\")\n\n    def compute_auc(self, y_true, y_pred, max_fpr=0.2):\n        v_gt = np.abs(y_true - 1)\n        v_pred = 1.0 - y_pred\n        partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n        partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n        return partial_auc\n\n    def compute_auc_kaggle(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32).numpy()\n        y_pred = tf.cast(y_pred, tf.float32).numpy()\n\n        # Invert classes as needed for minority focus\n        v_gt = 1 - y_true\n        v_pred = 1 - y_pred\n        fpr, tpr, _ = roc_curve(v_gt, v_pred)\n\n        # Find where FPR reaches max_fpr\n        stop = np.searchsorted(fpr, self.max_fpr, 'right')\n        if stop == 0:  # If max_fpr is very low and FPR doesn't reach it\n            return 0.0\n\n        x_interp = [fpr[stop - 1], fpr[stop]]\n        y_interp = [tpr[stop - 1], tpr[stop]]\n        tpr_interp = np.interp(self.max_fpr, x_interp, y_interp)\n\n        # Append this interpolated point and calculate AUC\n        tpr = np.append(tpr[:stop], tpr_interp)\n        fpr = np.append(fpr[:stop], self.max_fpr)\n        return auc(fpr, tpr)\n\n    def get_best_partial_auc(self):\n        return self.best_partial_auc\n\n    def get_best_epoch(self):\n        return self.best_epoch\n\n    def __deepcopy__(self, memo):\n        # Skip deep copying the validation data to avoid issues\n        return PartialAUCCallback(self.validation_data, self.validation_steps,\n                                  self.monitor, self.train_data,\n                                  self.steps_per_epoch, self.min_tpr)\n\nclass LearningRatePrinterCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        # Access the learning rate from the optimizer\n        lr = self.model.optimizer.learning_rate\n        # If the learning rate is a learning rate schedule (like CyclicalLearningRate), evaluate it\n        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n            lr = lr(self.model.optimizer.iterations)\n        tf.print(f\"\\nLearning rate at the end of epoch {epoch + 1}: {tf.keras.backend.get_value(lr)}\")\n\n\n# Custom Callback to monitor memory after each epoch\nclass MemoryUsageCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        process = psutil.Process(os.getpid())\n        mem = process.memory_info().rss / (1024 * 1024)  # RSS memory in MB\n        print(f\"Memory usage after epoch {epoch + 1}: {mem:.2f} MB\")\n        #tf.keras.backend.clear_session()\n        #tf.compat.v1.reset_default_graph()\n        gc.collect()\n        #clean_session()","metadata":{"id":"GqAeIwVPgdqu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizers","metadata":{"id":"9TNOAcVd6QWp"}},{"cell_type":"code","source":"def clean_metric_name(metric_name):\n    # Use regular expression to check if the string ends with _ followed by digits\n    if re.search(r'_\\d+$', metric_name):\n        # Remove the _ and the digits at the end\n        return re.sub(r'_\\d+$', '', metric_name)\n    return metric_name\n\n# Define a custom learning rate scheduler for cyclical learning rates\nclass CyclicalLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, initial_learning_rate, maximal_learning_rate, step_size):\n        self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n        self.maximal_learning_rate = tf.cast(maximal_learning_rate, tf.float32)\n        self.step_size = tf.cast(step_size, tf.float32)\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        cycle = tf.floor(1 + step / (2 * self.step_size))\n        x = tf.abs(step / self.step_size - 2 * cycle + 1)\n        lr = self.initial_learning_rate + (self.maximal_learning_rate - self.initial_learning_rate) * tf.maximum(0.0, (1 - x))\n        return lr\n\n    def get_config(self):\n        return {\n            \"initial_learning_rate\": self.initial_learning_rate.numpy(),\n            \"maximal_learning_rate\": self.maximal_learning_rate.numpy(),\n            \"step_size\": self.step_size.numpy(),\n        }\n\n# Set the parameters for the CLR\ninitial_learning_rate = 1e-6  # Minimum LR\nmaximal_learning_rate = 1e-4  # Maximum LR\nstep_size = 50  # Number of steps in half a cycle\ndef set_optimizer(optimizer, learning_rate=1e-4, use_nesterov_sgd=False, use_amsgrad_adam=False,\n                  initial_learning_rate=1e-6,\n                  maximal_learning_rate=1e-4,\n                  step_size=50\n                  ):\n    if optimizer == \"sgd\":\n        optimizer = optimizers.SGD(\n            learning_rate=learning_rate,\n            momentum=0.9,\n            nesterov=use_nesterov_sgd\n        )\n\n    elif optimizer == \"adam\":\n        optimizer = optimizers.Adam(\n            learning_rate=learning_rate,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=0.1,\n            amsgrad=use_amsgrad_adam\n        )\n\n    elif optimizer == \"nadam\":\n        optimizer = optimizers.Nadam(\n            learning_rate=learning_rate,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=0.1\n        )\n    elif optimizer == \"adamw\":\n        optimizer = optimizers.AdamW(\n            learning_rate=learning_rate,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=0.1\n        )\n    elif optimizer == \"adamax\":\n        optimizer = optimizers.Adamax(\n            learning_rate=learning_rate,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=0.1\n        )\n    elif optimizer == \"radam\":\n        optimizer = tfa.optimizers.RectifiedAdam(\n            learning_rate=learning_rate,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=0.1\n        )\n    elif optimizer == \"rmsprop\":\n        optimizer = optimizers.RMSprop(\n            learning_rate=learning_rate,\n            rho=0.9,\n            momentum=0.0,\n            epsilon=0.1,\n            centered=False\n        )\n    elif optimizer == \"nadam_cyclical\":\n         optimizer =  optimizers.Nadam(\n             learning_rate=CyclicalLearningRate(\n                 initial_learning_rate=initial_learning_rate,\n                 maximal_learning_rate=maximal_learning_rate,\n                 step_size=step_size\n                 )\n           )\n    elif optimizer == \"radam_cyclical\":\n         optimizer = tfa.optimizers.RectifiedAdam(\n             learning_rate=CyclicalLearningRate(\n                 initial_learning_rate=initial_learning_rate,\n                 maximal_learning_rate=maximal_learning_rate,\n                 step_size=step_size\n                 )\n           )\n    elif optimizer == \"adamw_cyclical\":\n         optimizer =  optimizers.AdamW(\n             learning_rate=CyclicalLearningRate(\n                 initial_learning_rate=initial_learning_rate,\n                 maximal_learning_rate=maximal_learning_rate,\n                 step_size=step_size\n                 )\n           )\n    return optimizer\n\nclass OneCycleScheduler(tf.keras.callbacks.Callback):\n    def __init__(self, max_lr, steps_per_epoch, epochs, start_lr=None, last_epoch_lr=None):\n        super(OneCycleScheduler, self).__init__()\n        self.max_lr = max_lr\n        self.steps_per_epoch = steps_per_epoch\n        self.epochs = epochs\n        self.total_steps = steps_per_epoch * epochs\n        self.current_step = 0\n\n        if start_lr is None:\n            self.start_lr = max_lr / 25  # A commonly used starting ratio\n        else:\n            self.start_lr = start_lr\n\n        if last_epoch_lr is None:\n            self.last_epoch_lr = self.start_lr / 1e4  # A small value to anneal towards\n        else:\n            self.last_epoch_lr = last_epoch_lr\n\n    def on_batch_begin(self, batch, logs=None):\n        # Update learning rate based on current step\n        self.current_step += 1\n        lr = self.calc_lr()\n        #tf.keras.backend.set_value(self.model.optimizer[0].lr, lr)\n        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n\n    def calc_lr(self):\n        # 1Cycle Learning Rate Schedule: Increases then decreases\n        step_ratio = self.current_step / self.total_steps\n        if step_ratio < 0.6:\n            # Increase learning rate for first half of training\n            lr = self.start_lr + (self.max_lr - self.start_lr) * (step_ratio * 2)\n        else:\n            # Decrease learning rate for the second half of training\n            lr = self.max_lr - (self.max_lr - self.last_epoch_lr) * ((step_ratio - 0.5) * 2)\n        return lr\n\nclass SGDRScheduler(tf.keras.callbacks.Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.'''\n    def __init__(self, min_lr, max_lr, steps_per_epoch, lr_decay=1, cycle_length=10, mult_factor=2):\n        super(SGDRScheduler, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n        self.steps_per_epoch = steps_per_epoch\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n    def on_batch_begin(self, batch, logs=None):\n        '''Adjust the learning rate at the start of each batch.'''\n        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n        self.batch_since_restart += 1\n\n    def on_epoch_end(self, epoch, logs=None):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.next_restart += self.cycle_length\n            self.cycle_length = int(self.cycle_length * self.mult_factor)\n            self.max_lr *= self.lr_decay\n            self.min_lr *= self.lr_decay","metadata":{"id":"Jer9dL9s0pIh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training utils","metadata":{"id":"7RPt5to1e2ru"}},{"cell_type":"markdown","source":"### Plotting metrics","metadata":{"id":"GY3gVK7Ze2ru"}},{"cell_type":"code","source":"def plot_training_history(history, model_name, metrics=['f1_score', 'precision', 'recall', 'loss', 'accuracy', 'partial_auc', 'auc']):\n    plt.figure(figsize=(16, 16))\n    for i, metric in enumerate(metrics):\n        plt.subplot(4, 2, i + 1)\n        plt.title(metric.capitalize())\n        if metric in history:\n            plt.plot(history[metric], label=metric)\n        if f'val_{metric}' in history:\n            plt.plot(history[f'val_{metric}'], label=f'val_{metric}')\n        plt.legend(loc='upper right')\n    plt.tight_layout()\n    plt.title(model_name, loc='center')\n    plt.savefig(f'{models_dir}/{model_name}_training_{run_id}')\n    plt.show()\n\ndef normalize_history_keys(history):\n    \"\"\"\n    Normalize the keys in the history dictionary using clean_metric_name.\n    \"\"\"\n    return {clean_metric_name(key): value for key, value in history.items()}\n\ndef print_infos(training_mode, fold_no, messages):\n    if training_mode == 'train-validation-split':\n        for message in messages:\n            print(message.capitalize())\n    else:\n        for message in messages:\n            print(f\"Fold {fold_no + 1}\", message)\n\ndef merge_histories(history_initial, history_fine_tune):\n    # Merge histories\n    history = defaultdict(list)\n    if history_initial is not None:\n        # Normalize the keys in history_initial\n        history_initial_cleaned = normalize_history_keys(history_initial.history)\n        for key, value in history_initial_cleaned.items():\n            history[key] = value\n    else:\n        # Normalize the keys in history_fine_tune\n        history_fine_tune_cleaned = normalize_history_keys(history_fine_tune.history)\n        for key, value in history_fine_tune_cleaned.items():\n            history[key].extend(value)\n    history = dict(history)\n    return history","metadata":{"id":"x_sTrPhlblvI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading","metadata":{"id":"eJivwO4ve2ru"}},{"cell_type":"code","source":"def load_train_val_dataset(file_pattern, shuffle_dataset_at_each_call=True, cache_dataset=True, cache_in_memory=False,\n                 cache_dir=cache_dir, prefix=\"train_val\", run_id=run_id, check_modification_time=False,\n                 is_training=True, force_cache=True, use_tabular_data=use_tabular_data, batch_size=batch_size):\n    print(\"Reloading and shuffling dataset...\")\n    try:\n        del dataset\n        gc.collect()\n    except:\n        pass\n        gc.collect()\n\n    print('batch_size', batch_size)\n    print('file_pattern', file_pattern)\n    print('is_training', is_training)\n\n    print('cache_in_memory', cache_in_memory)\n\n    # Load dataset once and cache\n    dataset, dataset_steps = load_tfrecord_dataset(file_pattern=file_pattern, is_training=is_training,\n                                                   batch_size=batch_size, cache_in_memory=cache_in_memory)\n\n    #dataset = cast_labels(dataset, use_tabular_data)\n\n\n    #dataset = persist_dataset(dataset, file_pattern=train_file_pattern, dataset_steps=dataset_steps, cache_in_memory=cache_in_memory,\n                    #cache_dir=cache_dir, prefix=prefix, run_id=run_id, check_modification_time=check_modification_time, force_cache=force_cache)\n\n    # Extract labels for stratified splitting\n    use_cross_validation = False\n    if use_cross_validation :\n        labels = []\n        for *_, lbl in dataset:\n            labels.append(lbl.numpy())\n\n        labels = np.array(labels)\n\n        seed = random.randint(0, 1e9) if shuffle_dataset_at_each_call else 42\n\n        if use_cross_validation:\n            # Generate stratified split indices for cross-validation\n            split_indices = stratified_split_indices(labels, n_splits=5, shuffle=True, random_state=seed)\n        else:\n            # Create a stratified 80/20 train-test split\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n            train_index, val_index = next(sss.split(np.zeros(len(labels)), labels))\n            split_indices = [(train_index, val_index)]  # Single split for train-test\n        return dataset, split_indices, dataset_steps\n    else:\n        return dataset, dataset_steps\n","metadata":{"id":"hYWdU0F-e2ru","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training pipeline common","metadata":{"id":"VNQ4pRa-yFtu"}},{"cell_type":"code","source":"def training_pipeline(model_name, model_names=None, model=None, train_file_pattern=None, **kwargs):\n    run_id = kwargs.get('run_id', None)\n    strategy = kwargs.get('strategy', None)\n    batch_size = kwargs.get('batch_size', None)\n    val_batch_size = kwargs.get('val_batch_size', None)\n    cache_in_memory = kwargs.get('cache_in_memory', None)\n    freeze_base_model = kwargs.get('freeze_base_model', None)\n    loss = kwargs.get('loss', None)\n    shuffle_dataset_at_each_call = kwargs.get('shuffle_dataset_at_each_call', None)\n    do_fine_tuning = kwargs.get('do_fine_tuning', None)\n    initial_epochs = kwargs.get('initial_epochs', None)\n    fine_tune_epochs = kwargs.get('fine_tune_epochs', None)\n    oversample_minority_class = kwargs.get('oversample_minority_class', None)\n    augment_train_data = kwargs.get('augment_train_data', None)\n    use_cross_validation = kwargs.get('use_cross_validation', None)\n    use_tabular_data = kwargs.get('use_tabular_data', None)\n    execution_env = kwargs.get('execution_env', None)\n    total_epochs = initial_epochs + fine_tune_epochs  if freeze_base_model else fine_tune_epochs\n\n    # cleaning the session\n    deletes_old_datasets()\n\n    partial_aucs = []\n    training_mode = 'train-validation-split'\n    print(\"Kwargs from training_pipeline \", kwargs)\n\n    # Load train dataset once and cache\n    train_dataset, training_steps = load_tfrecord_dataset(file_pattern=train_file_pattern,\n                                                        batch_size=batch_size,\n                                                        is_training=True,\n                                                        cache_in_memory=cache_in_memory)\n    # Validation dataset\n    val_dataset, validation_steps = load_tfrecord_dataset(f'{tf_records_eval_dir}/*.tfrecord',\n                                                                batch_size=val_batch_size,\n                                                                is_training=False,\n                                                                cache_in_memory=cache_in_memory)\n\n    print_infos(training_mode, None, [f\"start training model {model_name}...\", \n                                      f\"training size: {training_steps * batch_size * strategy.num_replicas_in_sync}\",\n                                      f\"validation size: {validation_steps * val_batch_size * strategy.num_replicas_in_sync}\"])\n\n    # Oversample the minority class within the fold\n    if oversample_minority_class:\n        (train_dataset, training_steps, training_total_samples, sample_per_class_input_ds,  \n        sample_per_class_oversampled_ds) = oversample_minority_class_random(train_dataset, batch_size, strategy)\n        print('Total samples in train dataset after oversampling', training_total_samples)\n\n    # Calculate class weights\n    class_weights_ovs = calculate_class_weights(sample_per_class_oversampled_ds)\n    class_weights_input_ds = calculate_class_weights(sample_per_class_input_ds)\n    print_infos(training_mode, None, [f\"samples per class: {sample_per_class_input_ds}\", f\"class weights: {class_weights_input_ds}\",\n                                            f\"training steps: {training_steps}, evaluation steps: {validation_steps}\"])\n\n    # Training data processing\n    train_dataset = optimize_dataset(train_dataset, file_pattern=None, steps=training_steps, cache_in_memory=cache_in_memory,\n                    cache_dir=cache_dir, prefix='train', run_id=run_id, check_modification_time=False, force_cache=True)\n\n    if augment_train_data:\n        train_dataset = apply_augmentations(train_dataset)\n    train_dataset = format_input_dataset(train_dataset, use_tabular_data)\n    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n\n    # Validation data processing\n    val_dataset = format_input_dataset(val_dataset, use_tabular_data)\n    val_dataset = optimize_dataset(val_dataset, file_pattern=None, steps=validation_steps, cache_in_memory=cache_in_memory,\n                    cache_dir=cache_dir, prefix='val', run_id=run_id, check_modification_time=False,\n                    force_cache=True)\n\n    # Callbacks\n    memory_callback = MemoryUsageCallback()\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    partial_auc_callback = PartialAUCCallback(validation_data=val_dataset, validation_steps=validation_steps,\n                                              monitor='val_partial_auc')\n    one_cycle_scheduler_callback = OneCycleScheduler(max_lr=1e-5, steps_per_epoch=training_steps, epochs=total_epochs,\n                                                     last_epoch_lr=1e-7, start_lr=1e-7)\n    model_checkpoint_callback =  ModelCheckpoint(filepath=f'{models_dir}/model_{model_name}_{run_id}_checkpoint.keras',\n                                                 monitor='val_partial_auc', save_best_only=True)\n    callbacks=[one_cycle_scheduler_callback, partial_auc_callback, early_stopping_callback, model_checkpoint_callback, memory_callback]\n\n\n    with strategy.scope():\n        precision = tf.keras.metrics.Precision()\n        recall = tf.keras.metrics.Recall()\n        f1_score = F1Score()\n        auc = tf.keras.metrics.AUC(name=\"auc\")\n\n        optimizer = tf.keras.optimizers.Nadam() if execution_env == 'colab' else tf.keras.optimizers.Adamax()\n        loss = set_binary_crossentropy_weighted_loss(positive_weights=np.array([class_weights_input_ds['minority_class']]),\n                                                    negative_weights=np.array([class_weights_input_ds['majority_class']]),\n                                                    epsilon=1e-7)\n    history_initial = None\n    class_weights=dict(zip([0, 1], class_weights_input_ds.values()))\n    # Initial training\n    if freeze_base_model:\n        with strategy.scope():\n            model.compile(optimizer=optimizer, loss=loss, metrics=[precision, recall, f1_score])\n        print(\"Started fitting the model with base model freezed...\")\n        history_initial = model.fit(train_dataset,epochs=initial_epochs,steps_per_epoch=training_steps, validation_data=val_dataset,\n                                    validation_steps=validation_steps, callbacks=callbacks, class_weight=class_weights, verbose=1)\n        \n    # Fine tuning\n    if do_fine_tuning:\n        initial_epoch = history_initial.epoch[-1] if freeze_base_model else 0\n\n        with strategy.scope():\n            model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', precision, recall, f1_score, auc])\n\n        print(\"Started fine fitting the model with layers unfrozen...\")\n        history_fine_tune = model.fit(train_dataset, epochs=total_epochs, initial_epoch=initial_epoch, steps_per_epoch=training_steps,\n                                      validation_data=val_dataset, validation_steps=validation_steps, class_weight=class_weights,\n                                      callbacks=callbacks, verbose=1)\n        # Merge histories\n        history = merge_histories(history_initial, history_fine_tune)\n    else:\n        history = history_initial.history\n\n    print(\"Computing the final validation score after training ended\")\n    # Evaluate the model on the validation set\n    y_val, y_pred = get_labels_and_predictions(model, val_dataset, validation_steps)\n\n    # Calculate the Partial AUC\n    partial_auc = score(y_val, y_pred)\n    partial_aucs.append(partial_auc)\n\n    print(\"Plotting the training metrics...\")\n    # Plot the training history\n    plot_training_history(history, model_name)\n\n    model.save_weights(os.path.join(models_dir, f'{model_name}_{run_id}_weights.h5'))\n    \n    print(f\"Train-validation-split partial AUC: {partial_auc}\")\n\n    print(f'Finished training model {model_name}')\n    return model","metadata":{"id":"LZImUCJeTqk2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Single model training pipeline","metadata":{"id":"-rb4sntpyFtu"}},{"cell_type":"code","source":"def train_single_model(model_name, **kwargs):\n    \"\"\"Train a unique model from scratch.\"\"\"\n\n    print(f\"Training single model: {model_name}\\n\")\n\n    model = ModelFactory.create_model('single', pretrained=False, model_names=[model_name], **kwargs)\n    model = training_pipeline(model_name, model=model, **kwargs)\n\n    return model","metadata":{"id":"ra_VML37yFtu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"5TaVBBPYe2ru"}},{"cell_type":"markdown","source":"### Utils","metadata":{"id":"F8h9mp1EyFtu"}},{"cell_type":"code","source":"class TrainConfig:\n    def __init__(self, strategy, run_id, batch_size, val_batch_size, cache_in_memory,\n                 freeze_base_model, shuffle_dataset_at_each_call, train_file_pattern,\n                 do_fine_tuning, oversample_minority_class, initial_epochs, fine_tune_epochs,\n                 img_shape, augment_train_data, use_cross_validation, dropout_rate, l2_lambda,\n                 num_tabular_features, use_tabular_data, kernel_initializer, activation,\n                 execution_env):\n        self.strategy = strategy\n        self.run_id = run_id\n        self.batch_size = batch_size\n        self.val_batch_size = val_batch_size\n        self.cache_in_memory = cache_in_memory\n        self.freeze_base_model = freeze_base_model\n        self.shuffle_dataset_at_each_call = shuffle_dataset_at_each_call\n        self.train_file_pattern = train_file_pattern\n        self.do_fine_tuning = do_fine_tuning\n        self.oversample_minority_class = oversample_minority_class\n        self.initial_epochs = initial_epochs\n        self.fine_tune_epochs = fine_tune_epochs\n        self.img_shape = img_shape\n        self.augment_train_data = augment_train_data\n        self.use_cross_validation = use_cross_validation\n        self.dropout_rate = dropout_rate\n        self.l2_lambda = l2_lambda\n        self.num_tabular_features = num_tabular_features\n        self.use_tabular_data = use_tabular_data\n        self.kernel_initializer = kernel_initializer\n        self.activation = activation\n        self.execution_env = execution_env\n\n\ndef run_pipeline(pipeline_func, config: TrainConfig, **specific_params):\n    \"\"\"General pipeline function for training models with a TrainConfig object.\"\"\"\n\n    # Clean the session and retrieve initial memory\n    initial_memory_usage = reset_session_and_get_memory()\n    log_memory_usage(\"Initial\", initial_memory_usage)\n\n    # Ensure strategy is initialized\n    if config.strategy is None:\n        raise ValueError(\"Strategy must be set in the configuration.\")\n\n    # Train the model using the passed function with the TrainConfig object\n    #with config.strategy.scope():\n    model = pipeline_func(strategy=config.strategy, run_id=config.run_id, batch_size=config.batch_size,\n                           cache_in_memory=config.cache_in_memory, freeze_base_model=config.freeze_base_model,\n                           shuffle_dataset_at_each_call=config.shuffle_dataset_at_each_call,\n                           do_fine_tuning=config.do_fine_tuning, train_file_pattern=config.train_file_pattern,\n                           oversample_minority_class = config.oversample_minority_class,\n                           initial_epochs = config.initial_epochs, val_batch_size = config.val_batch_size,\n                           fine_tune_epochs = config.fine_tune_epochs,\n                           img_shape = config.img_shape,\n                           augment_train_data = config.augment_train_data,\n                           use_cross_validation = config.use_cross_validation,\n                           use_tabular_data = config.use_tabular_data,\n                           activation = config.activation,\n                           dropout_rate = config.dropout_rate,\n                           l2_lambda = config.l2_lambda,\n                           num_tabular_features = config.num_tabular_features,\n                           kernel_initializer = config.kernel_initializer,\n                           execution_env = config.execution_env,\n                           **specific_params)\n    # Retrieve final memory usage\n    final_memory_usage = get_memory_usage()\n    log_memory_usage(\"Final\", final_memory_usage)\n\n    return model\n\ndef run_model_evaluation(model):\n    evaluate_model(model, val_dataset_small_formatted, validation_steps_small, dataset_name='small validation dataset')\n    evaluate_model(model, val_dataset_big_formatted, validation_steps_big, dataset_name='big validation dataset')\n    evaluate_model(model, train_val_dataset_formatted, train_val_dataset_steps, dataset_name='train_val dataset')","metadata":{"id":"ZT2DbKsMyFtu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialization","metadata":{"id":"mQa1JmVSe2ru"}},{"cell_type":"code","source":"# Instantiate TrainConfig with common parameters\ntrain_config = TrainConfig(\n    strategy=strategy,\n    run_id=run_id,\n    batch_size=batch_size,\n    val_batch_size=val_batch_size,\n    cache_in_memory=cache_in_memory,\n    freeze_base_model=freeze_base_model,\n    shuffle_dataset_at_each_call=True,\n    train_file_pattern=train_file_pattern,\n    do_fine_tuning = do_fine_tuning,\n    oversample_minority_class = oversample_minority_class,\n    initial_epochs = initial_epochs,\n    fine_tune_epochs = fine_tune_epochs,\n    img_shape = img_shape,\n    augment_train_data = augment_train_data,\n    use_cross_validation = use_cross_validation,\n    dropout_rate = dropout_rate,\n    l2_lambda = l2_lambda,\n    num_tabular_features = num_tabular_features,\n    use_tabular_data = use_tabular_data,\n    kernel_initializer = kernel_initializer,\n    activation = activation,\n    execution_env = execution_env\n)","metadata":{"id":"2TDM10k6yFtu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading","metadata":{"id":"47Qz3n7oe2rv"}},{"cell_type":"code","source":"# Load dataset once and cache\nif run_evaluations:\n    train_val_dataset, train_val_dataset_steps = load_tfrecord_dataset(\n            train_file_pattern,\n            batch_size=batch_size,\n            is_training=False,\n            cache_in_memory=False)\n    train_val_dataset_formatted = format_input_dataset(train_val_dataset)\n    # Verify cache\n    #!ls -al {cache_dir}\n    # Load big validation dataset\n    val_dataset_small, validation_steps_small = load_tfrecord_dataset(\n            f'{tf_records_eval_dir}/*.tfrecord',\n            batch_size=eval_batch_size,\n            is_training=False,\n            cache_in_memory=False)\n    val_dataset_small_formatted = format_input_dataset(val_dataset_small)\n\n    # Load big validation dataset\n    val_dataset_big, validation_steps_big = load_tfrecord_dataset(\n            f'{tf_records_val_dir}/*.tfrecord',\n            batch_size=eval_batch_size * 2,\n            is_training=False,\n            cache_in_memory=False)\n    val_dataset_big_formatted = format_input_dataset(val_dataset_big)","metadata":{"id":"RCMr7pPWSdJ5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{"id":"cSXVvmhpLkTY"}},{"cell_type":"code","source":"if model_type == 'single_model'  or train_all_models:\n    #strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    #strategy = tf.distribute.get_strategy()\n    #with strategy.scope():\n    single_model = run_pipeline(\n            pipeline_func=train_single_model, # Function to train the model\n            config=train_config,  # Common configuration object\n            #model_name='res_net_50'\n            model_name=model_name\n        )\n    if inference_model_name == 'single_model':\n         inference_model = single_model","metadata":{"id":"_V7fwzuxYKnn","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate on unseen and train_val datasets","metadata":{"id":"ferk6DB0e2rv"}},{"cell_type":"code","source":"if model_type == 'single_model' or train_all_models:\n    print('Evaluating single model:', model_name)\n    run_model_evaluation(single_model)","metadata":{"id":"16EUhZshe2rv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling models","metadata":{"id":"PUxNGwlOYZiB"}},{"cell_type":"markdown","source":"## Trained ensemble model","metadata":{}},{"cell_type":"markdown","source":"#### Trained ensemble training pipeline","metadata":{"id":"C6LwSwXEe2rw"}},{"cell_type":"code","source":"def train_compact_ensemble_model(model_names=['efficientnetb0', 'resnet50', 'vgg16'], **kwargs):\n    \"\"\"Train a unique ensmeble model from scratch.\"\"\"\n\n    print(f\"Training compact ensemble model\")\n\n    compact_ensemble_model = ModelFactory.create_model('ensemble', model_names, pretrained=False, **kwargs)\n\n    compact_ensemble_model = training_pipeline(compact_ensemble_model.model_name, model=compact_ensemble_model, **kwargs)\n    return compact_ensemble_model","metadata":{"id":"amV6RsxDe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run","metadata":{"id":"D_N4Ysa0fgX5"}},{"cell_type":"code","source":"if model_type == 'compact_ensemble_model' and train_all_models:\n    compact_ensemble_model = run_pipeline(\n        pipeline_func=train_compact_ensemble_model, # Function to train ensemble model\n        config=train_config,  # Common configuration object\n        #model_names=['densenet201','resnet50', 'resnet152v2','vgg19'],\n        #weights=[0.41,  0.35, 0.14, 0.1],\n        #weights=[0.25,  0.25, 0.25, 0.25],\n        model_names=['res_net_152_v2', 'dense_net_201'],\n        weights=[0.55,  0.45]\n    )","metadata":{"id":"52ZYRcZae2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation","metadata":{"id":"_I8Qh6vNe2rw"}},{"cell_type":"code","source":"if model_type == 'compact_ensemble_model' or train_all_models:\n    print('Evaluating compact ensemble model')\n    run_model_evaluation(compact_ensemble_model)\n\n    if inference_model_name == 'compact_ensemble_model':\n        inference_model = compact_ensemble_model","metadata":{"id":"EMHz-8tNe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained ensemble model","metadata":{"id":"_ww-Q4P3VYom"}},{"cell_type":"markdown","source":"### Pretrained ensemble pipeline","metadata":{"id":"lsk9qgwze2rw"}},{"cell_type":"code","source":"def train_pretrained_ensemble(model_names, train_individuals=True,  **kwargs):\n    \"\"\"Train a unique model from scratch.\"\"\"\n    print('Training pretrained ensemble model')\n\n    run_id = kwargs.get('run_id', None)\n\n    # Train individual models if needed\n    train_individual_models(model_names=model_names, train_individuals=train_individuals, **kwargs)\n\n    pretrained_ensemble = ModelFactory.create_model(model_type='ensemble', model_names=model_names, pretrained=True, **kwargs)\n\n    # Saving the weights as it is not trained via the commom pipeline\n    pretrained_ensemble.save_weights(os.path.join(models_dir, f'{pretrained_ensemble.model_name}_{run_id}_weights.h5'))\n\n    print('Finished creating the pretrained_ensemble', pretrained_ensemble.model_name)\n\n    return pretrained_ensemble\n\ndef train_individual_models(model_names, train_individuals=True, **kwargs):\n    \"\"\"Train individual models if their weights do not already exist.\"\"\"\n    for model_name in model_names:\n        print('Training individual model', model_name)\n        weights_path = os.path.join('models_dir', f'{model_name}_{kwargs.get(\"run_id\", \"default\")}_weights.h5')\n        if (not os.path.exists(weights_path)) and train_individuals:\n            print(f\"Training {model_name} since weights are not found.\")\n            train_single_model(model_name,  **kwargs)\n        else:\n            print(f\"Weights for {model_name} already exist at {weights_path}, skipping training.\")","metadata":{"id":"CraB6SdSe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"id":"uXGMYNGTe2rw"}},{"cell_type":"markdown","source":"### Run","metadata":{"id":"G4dJ5JLAe2rw"}},{"cell_type":"code","source":"if model_type == 'pretrained_ensemble_model' or train_all_models:\n    pretrained_ensemble_model = run_pipeline(\n        pipeline_func=train_pretrained_ensemble, # Function to train ensemble model\n        config=train_config,  # Common configuration object\n        model_names=['dense_net_201','res_net_50', 'res_net_152_v2','vgg_19'],\n        weights=[0.41,  0.35, 0.14, 0.1],\n        train_individuals=train_individuals,\n        mode='weighted'\n    )\n    if inference_model_name == 'pretrained_ensemble_model':\n        inference_model = pretrained_ensemble_model","metadata":{"id":"mi1rG5hGe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{"id":"WOJNkr42e2rw"}},{"cell_type":"code","source":"if (model_type == 'pretrained_ensemble_model' or train_all_models) and run_evaluations:\n    print('Evaluating pretrained ensemble model')\n    run_model_evaluation(pretrained_ensemble_model)","metadata":{"id":"1wmxQDwAe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"85VjJJ7uNHLk"}},{"cell_type":"markdown","source":"### Data preparation","metadata":{"id":"imjhThNXyFtv"}},{"cell_type":"code","source":"# Load and preprocess images using fused operations for I/O and preprocessing\n@tf.function\ndef preprocess_image(image_bytes, img_height, img_width):\n    \"\"\"Preprocess image loaded from HDF5.\"\"\"\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize(image, [img_height, img_width])\n    image = image / 255.0  # Normalize to [0, 1]\n    return image\n\ndef load_image_from_hdf5(key, h5_file):\n    \"\"\"Function to load image bytes from HDF5 file.\"\"\"\n    try:\n        image_bytes = h5_file[key][()]\n        if image_bytes is None or len(image_bytes) == 0:\n            raise ValueError(f\"Empty image data for key: {key}\")\n        return image_bytes\n    except Exception as e:\n        raise ValueError(f\"Failed to load image for key {key}. Error: {str(e)}\")\n\ndef hdf5_image_generator(h5_file, keys):\n    \"\"\"Python generator to load images from HDF5 file.\"\"\"\n    for key in keys:\n        try:\n            image_bytes = load_image_from_hdf5(key, h5_file)\n            yield image_bytes\n        except Exception as e:\n            print(f\"Error loading image for key {key}: {e}\")\n            continue\n\ndef prepare_inference_dataset(image_paths, data_source='hdf5', use_tabular_data = use_tabular_data, tabular_data_paths=None,\n                              batch_size=batch_size, cache_data=False, preprocessor_path=preprocessor_path, target_img_shape=None):\n    \"\"\"\n    Prepare the dataset for inference, either loading images from a file path or from an .hdf5 file.\n\n    Args:\n        image_source: Path to the images or .hdf5 file containing the images.\n        tabular_data_paths: Path to the tabular data, if any.\n        preprocessor: Preprocessing function for tabular data.\n        batch_size: Batch size for the dataset.\n        cache_data: Whether to cache the dataset.\n        data_source: 'path' if loading from image paths, 'hdf5' if loading from an .hdf5 file.\n\n    Returns:\n        A tf.data.Dataset ready for inference.\n    \"\"\"\n\n    # Loading images from file paths\n    if data_source == 'path':\n        # Fused operations for loading and preprocessing images from file paths\n        image_paths = glob(image_paths)\n        image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n\n        # Load and preprocess the images\n        image_dataset = image_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n\n        keys = [os.path.basename(path).split('.')[0] for path in image_paths]\n\n    # Loading images from an .hdf5 file\n    elif data_source == 'hdf5':\n        # Open the .hdf5 file and load the image dataset\n        # Open the HDF5 file\n        h5_file = h5py.File(image_paths, 'r')\n        keys = list(h5_file.keys())\n\n        # Create a generator for loading images from the HDF5 file\n        def image_gen():\n            return hdf5_image_generator(h5_file, keys)\n\n        # Create a TensorFlow Dataset from the generator\n        image_dataset = tf.data.Dataset.from_generator(\n            image_gen,\n            output_signature=tf.TensorSpec(shape=(), dtype=tf.string)\n        )\n\n        # Map preprocessing function over the dataset\n        image_dataset = image_dataset.map(\n            lambda image_bytes: preprocess_image(image_bytes, img_height, img_width),\n            num_parallel_calls=tf.data.AUTOTUNE\n        )\n\n    # Raise an error if the data_source is not recognized\n    else:\n        raise ValueError(\"data_source must be either 'path' or 'hdf5'\")\n\n    # If tabular data is provided, load and zip it with the image dataset\n    if use_tabular_data:\n        # Extract filenames without extensions\n        inference_images = keys\n        preprocessor = load_preprocessor(preprocessor_path)\n        tabular_data = format_and_encode_tabular_data([tabular_data_paths], inference_images, preprocessor)\n\n        tabular_data = tf.data.Dataset.from_tensor_slices(tabular_data)\n\n        # Combine image and tabular data efficiently\n        combined_dataset = tf.data.Dataset.zip((image_dataset, tabular_data))\n    else:\n        combined_dataset = image_dataset\n\n    # Batch the dataset and prefetch to load data asynchronously\n    dataset_for_inference = combined_dataset.batch(batch_size * strategy.num_replicas_in_sync,\n                                                   drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n    if cache_data:\n        dataset_for_inference = dataset_for_inference.cache()\n\n    return dataset_for_inference, keys","metadata":{"id":"rh4Se0aVOLQM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{"id":"_ylysfHoyFtw"}},{"cell_type":"code","source":"# Run inference using fused operations, mixed precision, and optimized parallel data loading\ndef inference_pipeline(model, image_paths, use_tabular_data, tabular_data_paths=None, batch_size=batch_size,\n                  cache_data=False, use_mixed_precision=False, preprocessor_path=preprocessor_path,\n                 submission_file_path=None, target_img_shape=None):\n    use_mixed_precision=False\n    if use_mixed_precision:\n        enable_mixed_precision()\n\n    # Prepare the dataset\n    dataset_for_inference, images_ids= prepare_inference_dataset(image_paths=image_paths, use_tabular_data=use_tabular_data,\n                                                      cache_data=cache_data, tabular_data_paths=tabular_data_paths,\n                                                      batch_size=batch_size, preprocessor_path=preprocessor_path,\n                                                      target_img_shape=target_img_shape)\n    all_predictions = []\n    # Run inference and get predictions\n    for batch in dataset_for_inference:\n        if use_tabular_data:\n            image_batch, tabular_batch = batch\n            predictions = model.predict_on_batch([image_batch, tabular_batch]).ravel()\n        else:\n            image_batch = batch\n            predictions = model.predict_on_batch(image_batch).ravel()\n        all_predictions.append(predictions)\n\n    # Save predictions to CSV\n    predictions_df = pd.DataFrame(zip(images_ids, predictions), columns=['isic_id', 'target'])\n    predictions_df.to_csv(submission_file_path, index=False)\n\n    # Process predictions\n    return predictions_df, dataset_for_inference","metadata":{"id":"JubaCa3ayFtw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialization","metadata":{"id":"VX1Pz5GmOaC8"}},{"cell_type":"code","source":"preprocessor_path = preprocessor_path\ninference_model_path = inference_model_path\nproject_dir = f'{input_dir}/isic-2024-challenge'\ninference_images_path = f'{project_dir}/test-image.hdf5'\ninference_tabular_data_path = f'{project_dir}/test-metadata.csv'\nuse_tabular_data = use_tabular_data\ncache_data=True\nsubmission_file_path = f'{output_dir}/submission.csv'\ntarget_img_shape=img_shape\nrun_inference_use_all_models = False","metadata":{"id":"68W0wKs5Oe26","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run","metadata":{"id":"xcvznEl-OkYP"}},{"cell_type":"code","source":"# Run inference with optimizations and mixed precision enabled\nif run_inference:\n    inference_models = [single_model, compact_ensemble_model, pretrained_ensemble_model] if run_inference_use_all_models else [inference_model]\n    predictions_dfs = []\n    for inf_model in inference_models:\n        print('Running inference for model...', inf_model.model_name)\n        predictions_df, dataset_for_inference = inference_pipeline(model=inf_model, image_paths=inference_images_path, use_tabular_data=use_tabular_data,\n                                      tabular_data_paths=inference_tabular_data_path, batch_size=batch_size,  cache_data=cache_data,\n                                      use_mixed_precision=False, preprocessor_path=preprocessor_path, submission_file_path=submission_file_path,\n                                      target_img_shape=target_img_shape)\n        predictions_dfs.append(predictions_df)\n        print('Finished running inference for model', inf_model.model_name)","metadata":{"id":"m4pqr3A2NJd-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_dfs[-1]","metadata":{"id":"CzavW04_RbHv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Annexe","metadata":{"id":"ao5HqGmRe2rw"}},{"cell_type":"code","source":"# Limit intra-op parallelism (operations within a single layer)\n#export OMP_NUM_THREADS=100\n# Limit inter-op parallelism (parallel operations across layers)\n#export TF_NUM_INTEROP_THREADS=100\n# Limit intra-op parallelism (for operations like matrix multiplications)\n#tf.config.threading.set_intra_op_parallelism_threads(100)\n# Limit inter-op parallelism (for parallel operations across layers)\n#tf.config.threading.set_inter_op_parallelism_threads(100)\n#import os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disables GPU by setting this environment variable\n#tf.config.list_physical_devices('GPU')","metadata":{"id":"nsXlSSMSe2rw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualization = False\nif visualization:\n    eval_dataset_mapped = eval_dataset.map(lambda x, y, z: ((x, y), z))\n    augmented_dataset = apply_augmentations(eval_dataset_mapped)\n    imgs = augmented_dataset.as_numpy_iterator()\n    imgs_t = imgs.next()[0][0]\n    plt.figure(figsize=(50, 50))\n    #imgs_t = imgs.next()[0]\n    for i in range(0, 63):\n        plt.subplot(12, 12, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(imgs_t[i])","metadata":{"id":"QqMgKK62e2rw","trusted":true},"execution_count":null,"outputs":[]}]}